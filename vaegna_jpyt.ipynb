{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE-GNA Jupyter Notebook\n",
    "---------------------------------\n",
    "\n",
    "**Autor:** Matheus Becali Rocha \\\n",
    "**Email:** matheusbecali@gmail.com\n",
    "\n",
    "\n",
    "#### Notebook Summary\n",
    "\n",
    "This notebook explores Variational Autoencoders (VAEs) and includes various functions, models, and optimization techniques.\n",
    "\n",
    "Setup and Imports:\n",
    "- Imports necessary libraries like `os`, `optuna`, `torch`, and `pyspectra` to handle data, models, and optimization.\n",
    "\n",
    "Auxiliary Functions:\n",
    "- **save_checkpoint**: Saves the current state of the model to a file.\n",
    "- **SNV**: Standard Normal Variate function to normalize data.\n",
    "- **idx2onehot**: Converts index values to one-hot encoded vectors for categorical data.\n",
    "\n",
    "Loss Functions:\n",
    "- **focal_loss**: A loss function used to handle class imbalance by focusing on hard-to-classify examples.\n",
    "- **adaptative_focal_loss**: An adaptive version of the focal loss function.\n",
    "\n",
    "Data Preparation:\n",
    "- Sets up data handling by defining constants and loading the necessary datasets.\n",
    "- Configures settings for data visualization to plot and analyze data distributions.\n",
    "\n",
    "Model Definitions:\n",
    "- **prepare_data_loader**: Prepares the data loader for feeding data into the model.\n",
    "- **ClassifyingNetwork**: Defines a Multi-Layer Perceptron (MLP) network for classifying data.\n",
    "- **AttentionLayer**: Implements an attention mechanism to improve model focus on important features.\n",
    "\n",
    "Optimization with Optuna:\n",
    "- **optuna_run**: Runs an optimization process using Optuna to find the best hyperparameters for the model.\n",
    "\n",
    "Experiment Runs:\n",
    "- Defines hyperparameters and timings for running multiple experiments.\n",
    "- Includes scripts to execute and visualize results for various models and configurations.\n",
    "\n",
    "Result Visualization:\n",
    "- Plots reconstructed data to visualize how well the model has learned to replicate input data.\n",
    "- Plots loss curves to show the model's training progress and performance over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import matplotlib\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "# matplotlib.style.use('dark_background')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "# Set the device to GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "try:\n",
    "    # Print the name of the CUDA device if available\n",
    "    print(torch.cuda.get_device_name(device))\n",
    "except Exception as e:\n",
    "    # Handle the exception if CUDA device is not found\n",
    "    print('CUDA device not found, using CPU instead.')\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "_seed = 78645\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading some functions and dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "def save_results_to_csv(filename, results_data, header):\n",
    "    \"\"\"\n",
    "    Save results data to a CSV file. If the file does not exist, create it and write the header.\n",
    "\n",
    "    Parameters:\n",
    "    filename (str): The name of the file to which the results will be saved.\n",
    "    results_data (dict): The results data to be written to the CSV file.\n",
    "    header (list): A list of strings representing the header of the CSV file.\n",
    "    \"\"\"\n",
    "    # Check if the file already exists\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    # Open the file in append mode\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        # Create a CSV DictWriter object\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=header)\n",
    "\n",
    "        # If the file does not exist, write the header\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Write the results data to the CSV file\n",
    "        writer.writerow(results_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNV(input_data):\n",
    "    \"\"\"\n",
    "    Standard Normal Variate (SNV) transformation: subtracts the row mean from each row and scales to unit variance.\n",
    "\n",
    "    Parameters:\n",
    "    input_data (pd.DataFrame): Input data with rows as samples and columns as features.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Transformed data with SNV applied.\n",
    "    \"\"\"\n",
    "    input_data = input_data.to_numpy()\n",
    "    # Define a new array and populate it with the corrected data  \n",
    "    output_data = np.zeros_like(input_data)\n",
    "    for i in range(input_data.shape[1]):\n",
    "        # Apply SNV correction\n",
    "        output_data[:, i] = (input_data[:, i] - np.mean(input_data[:, i])) / np.std(input_data[:, i], ddof=1)\n",
    "    \n",
    "    return output_data\n",
    "\n",
    "def LOaO(X):\n",
    "    \"\"\"\n",
    "    Linear Offset and Amplitude Scaling (LOaO): scales data to the range [-1, 1].\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame or np.ndarray): Input data.\n",
    "\n",
    "    Returns:\n",
    "    np.ndarray: Transformed data with values scaled to the range [-1, 1].\n",
    "    \"\"\"\n",
    "    return 2 * ((X - X.min()) / (X.max() - X.min())) - 1\n",
    "\n",
    "\n",
    "# Mofificado do Pyspectra\n",
    "\n",
    "class GeneralTransformer:\n",
    "    \"\"\"\n",
    "    A general transformer class for data preprocessing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, spc):\n",
    "        \"\"\"\n",
    "        Fit the transformer to the data.\n",
    "\n",
    "        Parameters:\n",
    "        spc (pd.DataFrame): Input data with rows as samples and columns as features.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def transform(self, spc):\n",
    "        \"\"\"\n",
    "        Transform the data using the fitted parameters.\n",
    "\n",
    "        Parameters:\n",
    "        spc (pd.DataFrame): Input data with rows as samples and columns as features.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: Transformed data.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit_transform(self, spc):\n",
    "        \"\"\"\n",
    "        Fit the transformer to the data and then transform it.\n",
    "\n",
    "        Parameters:\n",
    "        spc (pd.DataFrame): Input data with rows as samples and columns as features.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: Transformed data.\n",
    "        \"\"\"\n",
    "        self.fit(spc)\n",
    "        return self.transform(spc)\n",
    "\n",
    "class SNVTransformer(GeneralTransformer):\n",
    "    \"\"\"\n",
    "    SNVTransformer: A transformer for performing Standard Normal Variate (SNV) transformation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the SNVTransformer.\n",
    "        \"\"\"\n",
    "        self.MeanSpectra = None\n",
    "        self.StdSpectra = None\n",
    "\n",
    "    def fit(self, spc):\n",
    "        \"\"\"\n",
    "        Calculate the mean and standard deviation for SNV transformation.\n",
    "\n",
    "        Parameters:\n",
    "        spc (pd.DataFrame): Input data with rows as samples and columns as features.\n",
    "        \"\"\"\n",
    "        self.MeanSpectra = spc.mean(axis=0)\n",
    "        self.StdSpectra = spc.std(axis=0)\n",
    "\n",
    "    def transform(self, spc):\n",
    "        \"\"\"\n",
    "        Apply SNV transformation to the data.\n",
    "\n",
    "        Parameters:\n",
    "        spc (pd.DataFrame): Input data with rows as samples and columns as features.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: Transformed data with SNV applied.\n",
    "        \"\"\"\n",
    "        return (spc - self.MeanSpectra) / self.StdSpectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2onehot(idx, n):\n",
    "    \"\"\"\n",
    "    Convert indices to one-hot encoded vectors.\n",
    "\n",
    "    Parameters:\n",
    "    idx (torch.Tensor): Tensor containing indices.\n",
    "    n (int): Number of classes.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: One-hot encoded tensor.\n",
    "    \"\"\"\n",
    "    assert torch.max(idx).item() < n\n",
    "\n",
    "    if idx.dim() == 1:\n",
    "        idx = idx.unsqueeze(1)\n",
    "    \n",
    "    onehot = torch.zeros(idx.size(0), n).to(idx.device)\n",
    "    onehot.scatter_(1, idx, 1)\n",
    "    \n",
    "    return onehot\n",
    "\n",
    "def plot_gallery(spectre, epoch, fold, model_name, n_row=3, n_col=6, all_plot=False):\n",
    "    \"\"\"\n",
    "    Plot a gallery of spectre data.\n",
    "\n",
    "    Parameters:\n",
    "    spectre (list of torch.Tensor): List of tensors containing spectre data.\n",
    "    epoch (int): Current epoch number.\n",
    "    fold (int): Current fold number.\n",
    "    model_name (str): Name of the model.\n",
    "    n_row (int, optional): Number of rows in the plot. Defaults to 3.\n",
    "    n_col (int, optional): Number of columns in the plot. Defaults to 6.\n",
    "    all_plot (bool, optional): Flag indicating whether to plot all data or not. Defaults to False.\n",
    "    \"\"\"\n",
    "    \n",
    "    eixo_x = np.arange(125)\n",
    "    # eixo_x = np.arange(1401) # BCA cancer\n",
    "    # eixo_x = np.arange(22) # PCA-BCA cancer\n",
    "    # eixo_x = np.arange(1557) # Urine 1557 w/o derivate\n",
    "    # eixo_x = np.arange(1555) # Urine 1555 w derivate\n",
    "\n",
    "    if all_plot:\n",
    "        plt.figure(figsize=(8 * n_col, 8 * n_row))\n",
    "        for i in range(spectre[0].size()[0]):\n",
    "            for j in range(n_row * n_col):\n",
    "                ax = plt.subplot(n_row, n_col, j + 1)\n",
    "                if j == 0: \n",
    "                    ax.set_title(\"Input Data\")\n",
    "                else:\n",
    "                    ax.set_title(\"Recon Data\")\n",
    "                ax.axis(\"off\")\n",
    "                ax.plot(eixo_x, spectre[j][i].squeeze(0))\n",
    "        plt.savefig(f'imgs/{model_name}/image_at_epoch_{epoch:04d}_fold_{str(fold)}.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.figure(figsize=(2 * n_col, 2 * n_row))\n",
    "        for i in range(n_row * n_col):\n",
    "            ax = plt.subplot(n_row, n_col, i + 1)\n",
    "            if i == 0: \n",
    "                ax.set_title(\"Input Data\")\n",
    "            else:\n",
    "                ax.set_title(\"Recon Data\")\n",
    "            ax.axis(\"off\")\n",
    "            ax.plot(eixo_x, spectre[i][0].squeeze(0))\n",
    "\n",
    "        plt.savefig(f'imgs/{model_name}/image_at_epoch_{epoch:04d}_fold_{str(fold)}.png')\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "\n",
    "def expected_sigm_of_norm(\n",
    "    mean: torch.Tensor, std: torch.Tensor, method = 'probit'\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"\n",
    "    Approximate the expected value of the sigmoid of a normal distribution.\n",
    "\n",
    "    Thanks go to this guy:\n",
    "\n",
    "    https://math.stackexchange.com/questions/207861/expected-value-of-applying-the-sigmoid-function-to-a-normal-distribution    \n",
    "\n",
    "    Parameters:\n",
    "    mean (torch.Tensor): Mean of the normal distribution.\n",
    "    std (torch.Tensor): Standard deviation of the normal distribution.\n",
    "    method (str): Method to use for the approximation. Options are 'probit', 'maclauren-2', 'maclauren-3'.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: An approximation to `E(sigmoid(N(mean, std**2)))`.\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'maclauren-2':\n",
    "        eu = torch.exp(-mean)\n",
    "        approx_exp = 1/(eu + 1) + 0.5*(eu - 1)*eu / ((eu+1)**3) * std**2\n",
    "        return torch.clamp(approx_exp, min=0, max=1)\n",
    "    elif method == 'maclauren-3':\n",
    "        eu = torch.exp(-mean)\n",
    "        approx_exp = 1/(eu + 1) + 0.5*(eu - 1)*eu / ((eu + 1)**3) * std**2 + (eu**3 - 11*(eu**2) + 11*eu - 1) / ((8*(eu + 1))**5) * std**4\n",
    "        return torch.clamp(approx_exp, min=0, max=1)\n",
    "    elif method == 'probit':\n",
    "        # lambd = 0.61 # lambda approx 0.61, suggests in article https://arxiv.org/abs/1703.00091\n",
    "        lambd = np.sqrt(np.pi / 8)\n",
    "        dist = Normal(loc = 0, scale = 1)\n",
    "        return dist.cdf(mean.clone() / torch.sqrt(1 / (lambd ** 2) + std.clone() ** 2))\n",
    "    else:\n",
    "        raise Exception('Method \"% s\" not known' % method)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(\n",
    "    input: torch.Tensor, target: torch.Tensor, alpha: float, gamma: float = 2.0, reduction: str = 'none'\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"Criterion that computes Focal loss.\n",
    "    According to :cite:`lin2018focal`, the Focal loss is computed as follows:\n",
    "    .. math::\n",
    "        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n",
    "    Where:\n",
    "       - :math:`p_t` is the model's estimated probability for each class.\n",
    "    Args:\n",
    "        input: logits tensor with shape :math:`(N, C, *)` where C = number of classes.\n",
    "        target: labels tensor with shape :math:`(N, *)` where each value is :math:`0 ≤ targets[i] ≤ C−1`.\n",
    "        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n",
    "        gamma: Focusing parameter :math:`\\gamma >= 0`.\n",
    "        reduction: Specifies the reduction to apply to the\n",
    "          output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction\n",
    "          will be applied, ``'mean'``: the sum of the output will be divided by\n",
    "          the number of elements in the output, ``'sum'``: the output will be\n",
    "          summed.\n",
    "        eps: Deprecated: scalar to enforce numerical stabiliy. This is no longer used.\n",
    "    Return:\n",
    "        the computed loss.\n",
    "    Example:\n",
    "        >>> N = 5  # num_classes\n",
    "        >>> input = torch.randn(1, N, 3, 5, requires_grad=True)\n",
    "        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)\n",
    "        >>> output = focal_loss(input, target, alpha=0.5, gamma=2.0, reduction='mean')\n",
    "        >>> output.backward()\n",
    "\n",
    "    Thanks: https://github.com/zhezh/focalloss/blob/master/focalloss.py\n",
    "    \"\"\"\n",
    "\n",
    "    # compute softmax over the classes axis\n",
    "    input_soft = torch.softmax(input, dim=1)\n",
    "    log_input_soft = torch.log_softmax(input, dim=1)\n",
    "\n",
    "    # create the labels one hot tensor\n",
    "    target_one_hot = torch.nn.functional.one_hot(target, num_classes=input.shape[1]).type(torch.FloatTensor).to(input.device)\n",
    "\n",
    "    # compute the actual focal loss\n",
    "    weight = torch.pow(-input_soft + 1.0, gamma)\n",
    "    focal = -alpha * weight * log_input_soft\n",
    "    loss_tmp = torch.einsum('...bc,...bc->...', (target_one_hot, focal))\n",
    "\n",
    "    if reduction == 'none':\n",
    "        loss = loss_tmp\n",
    "    elif reduction == 'mean':\n",
    "        loss = torch.mean(loss_tmp)\n",
    "    elif reduction == 'sum':\n",
    "        loss = torch.sum(loss_tmp)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Invalid reduction mode: {reduction}\")\n",
    "    return loss\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    r\"\"\"Criterion that computes Focal loss.\n",
    "    According to :cite:`lin2018focal`, the Focal loss is computed as follows:\n",
    "    .. math::\n",
    "        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n",
    "    Where:\n",
    "       - :math:`p_t` is the model's estimated probability for each class.\n",
    "    Args:\n",
    "        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n",
    "        gamma: Focusing parameter :math:`\\gamma >= 0`.\n",
    "        reduction: Specifies the reduction to apply to the\n",
    "          output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction\n",
    "          will be applied, ``'mean'``: the sum of the output will be divided by\n",
    "          the number of elements in the output, ``'sum'``: the output will be\n",
    "          summed.\n",
    "        eps: Deprecated: scalar to enforce numerical stability. This is no longer\n",
    "          used.\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, *)` where C = number of classes.\n",
    "        - Target: :math:`(N, *)` where each value is\n",
    "          :math:`0 ≤ targets[i] ≤ C−1`.\n",
    "    Example:\n",
    "        >>> N = 5  # num_classes\n",
    "        >>> kwargs = {\"alpha\": 0.5, \"gamma\": 2.0, \"reduction\": 'mean'}\n",
    "        >>> criterion = FocalLoss(**kwargs)\n",
    "        >>> input = torch.randn(1, N, 3, 5, requires_grad=True)\n",
    "        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)\n",
    "        >>> output = criterion(input, target)\n",
    "        >>> output.backward()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, alpha: float, gamma: float = 2.0, reduction: str = 'none') -> None:\n",
    "        super().__init__()\n",
    "        self.alpha: float = alpha\n",
    "        self.gamma: float = gamma\n",
    "        self.reduction: str = reduction\n",
    "\n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        return focal_loss(input, target, self.alpha, self.gamma, self.reduction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptative Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptative_focal_loss(\n",
    "    input: torch.Tensor, target: torch.Tensor, beta: torch.Tensor, alpha: float, gamma_start: float = 2.0, reduction: str = 'none'\n",
    ") -> torch.Tensor:\n",
    "    r\"\"\"Criterion that computes Adaptative Focal loss.\n",
    "    According to :cite:`lin2018focal`, the Focal loss is computed as follows:\n",
    "    .. math::\n",
    "        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n",
    "    Where:\n",
    "       - :math:`p_t` is the model's estimated probability for each class.\n",
    "    Args:\n",
    "        input: logits tensor with shape :math:`(N, C, *)` where C = number of classes.\n",
    "        target: labels tensor with shape :math:`(N, *)` where each value is :math:`0 ≤ targets[i] ≤ C−1`.\n",
    "        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n",
    "        gamma: Focusing parameter :math:`\\gamma >= 0`.\n",
    "        reduction: Specifies the reduction to apply to the\n",
    "          output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction\n",
    "          will be applied, ``'mean'``: the sum of the output will be divided by\n",
    "          the number of elements in the output, ``'sum'``: the output will be\n",
    "          summed.\n",
    "        eps: Deprecated: scalar to enforce numerical stabiliy. This is no longer used.\n",
    "    Return:\n",
    "        the computed loss.\n",
    "    Example:\n",
    "        >>> N = 5  # num_classes\n",
    "        >>> input = torch.randn(1, N, 3, 5, requires_grad=True)\n",
    "        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)\n",
    "        >>> output = adaptative_focal_loss(input, target, alpha=0.5, gamma=2.0, reduction='mean')\n",
    "        >>> output.backward()\n",
    "    \"\"\"\n",
    "\n",
    "    # compute softmax over the classes axis\n",
    "    input_soft = torch.softmax(input, dim=1)\n",
    "    log_input_soft = torch.log_softmax(input, dim=1)\n",
    "\n",
    "    # create the labels one hot tensor\n",
    "    target_one_hot = torch.nn.functional.one_hot(target, num_classes=input.shape[1]).type(torch.FloatTensor).to(input.device)\n",
    "\n",
    "    # compute the actual focal loss\n",
    "    gamma_epoch = (gamma_start / torch.sqrt(beta)).to(input.device)\n",
    "    weight = torch.pow(-input_soft + 1.0, gamma_epoch)\n",
    "    focal = -alpha * weight * log_input_soft\n",
    "    loss_tmp = torch.einsum('...bc,...bc->...', (target_one_hot, focal))\n",
    "\n",
    "    if reduction == 'none':\n",
    "        loss = loss_tmp\n",
    "    elif reduction == 'mean':\n",
    "        loss = torch.mean(loss_tmp)\n",
    "    elif reduction == 'sum':\n",
    "        loss = torch.sum(loss_tmp)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Invalid reduction mode: {reduction}\")\n",
    "    return loss\n",
    "\n",
    "\n",
    "class AdaptativeFocalLoss(nn.Module):\n",
    "    r\"\"\"Criterion that computes Adaptative Focal loss.\n",
    "    According to :cite:`lin2018focal`, the Focal loss is computed as follows:\n",
    "    .. math::\n",
    "        \\text{FL}(p_t) = -\\alpha_t (1 - p_t)^{\\gamma} \\, \\text{log}(p_t)\n",
    "    Where:\n",
    "       - :math:`p_t` is the model's estimated probability for each class.\n",
    "    Args:\n",
    "        alpha: Weighting factor :math:`\\alpha \\in [0, 1]`.\n",
    "        gamma: Focusing parameter :math:`\\gamma >= 0`.\n",
    "        reduction: Specifies the reduction to apply to the\n",
    "          output: ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction\n",
    "          will be applied, ``'mean'``: the sum of the output will be divided by\n",
    "          the number of elements in the output, ``'sum'``: the output will be\n",
    "          summed.\n",
    "        eps: Deprecated: scalar to enforce numerical stability. This is no longer\n",
    "          used.\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, *)` where C = number of classes.\n",
    "        - Target: :math:`(N, *)` where each value is\n",
    "          :math:`0 ≤ targets[i] ≤ C−1`.\n",
    "    Example:\n",
    "        >>> N = 5  # num_classes\n",
    "        >>> kwargs = {\"alpha\": 0.5, \"gamma\": 2.0, \"reduction\": 'mean'}\n",
    "        >>> criterion = FocalLoss(**kwargs)\n",
    "        >>> input = torch.randn(1, N, 3, 5, requires_grad=True)\n",
    "        >>> target = torch.empty(1, 3, 5, dtype=torch.long).random_(N)\n",
    "        >>> output = criterion(input, target)\n",
    "        >>> output.backward()\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(self, alpha: float, gamma: float = 2.0, reduction: str = 'none') -> None:\n",
    "        super().__init__()\n",
    "        self.alpha: float = alpha\n",
    "        self.gamma: float = gamma\n",
    "        self.reduction: str = reduction\n",
    "\n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor, beta: torch.Tensor) -> torch.Tensor:\n",
    "        return adaptative_focal_loss(input, target, beta, self.alpha, self.gamma, self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspectra.transformers.spectral_correction import msc, snv, detrend, derivative, sav_gol\n",
    "\n",
    "def preprocessing(data, NORM, _normalization, _train=True, d=1):\n",
    "    \"\"\"\n",
    "    Preprocess the data using various normalization techniques.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame or np.ndarray): Input data to be normalized.\n",
    "    NORM (object): Normalization object to be used for fitting and transforming.\n",
    "    _normalization (str): Type of normalization to be applied.\n",
    "    _train (bool, optional): Flag indicating if the normalization should be fitted. Defaults to True.\n",
    "    d (int, optional): Derivative order for 'derivate' and 'Sav_Gol' normalization. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Transformed data and normalization object.\n",
    "    \"\"\"\n",
    "    if _normalization == \"SNV\":\n",
    "        print(\"Normalization SNV has been choice!\")\n",
    "        # Implemented\n",
    "        # X_data = SNV(data)\n",
    "\n",
    "        # Pyspectra\n",
    "        if _train:\n",
    "            X_data = NORM.fit_transform(data)\n",
    "        else:\n",
    "            X_data = NORM.transform(data)\n",
    "    elif _normalization == \"MSC\":\n",
    "        MSC= msc()\n",
    "        MSC.fit(data)\n",
    "        X_data=MSC.transform(data)\n",
    "    elif _normalization == \"SNV_Detrend\":\n",
    "        SNV= snv()\n",
    "        X_data=SNV.fit_transform(data)\n",
    "\n",
    "        Detr= detrend()\n",
    "        X_data.columns = [\n",
    "            908.1, 914.294, 920.489, 926.683, 932.877, 939.072, \n",
    "            945.266, 951.46, 957.655, 963.849, 970.044, 976.238, \n",
    "            982.432, 988.627, 994.821, 1001.015, 1007.21, 1013.404, \n",
    "            1019.598, 1025.793, 1031.987, 1038.181, 1044.376, 1050.57, \n",
    "            1056.764, 1062.959, 1069.153, 1075.348, 1081.542, 1087.736, \n",
    "            1093.931, 1100.125, 1106.319, 1112.514, 1118.708, 1124.902, \n",
    "            1131.097, 1137.291, 1143.485, 1149.68, 1155.874, 1162.069, \n",
    "            1168.263, 1174.457, 1180.652, 1186.846, 1193.04, 1199.235, \n",
    "            1205.429, 1211.623, 1217.818, 1224.012, 1230.206, 1236.401, \n",
    "            1242.595, 1248.789, 1254.984, 1261.178, 1267.373, 1273.567, \n",
    "            1279.761, 1285.956, 1292.15, 1298.344, 1304.539, 1310.733, \n",
    "            1316.927, 1323.122, 1329.316, 1335.51, 1341.705, 1347.899, \n",
    "            1354.094, 1360.288, 1366.482, 1372.677, 1378.871, 1385.065, \n",
    "            1391.26, 1397.454, 1403.648, 1409.843, 1416.037, 1422.231, \n",
    "            1428.426, 1434.62, 1440.814, 1447.009, 1453.203, 1459.398, \n",
    "            1465.592, 1471.786, 1477.981, 1484.175, 1490.369, 1496.564, \n",
    "            1502.758, 1508.952, 1515.147, 1521.341, 1527.535, 1533.73, \n",
    "            1539.924, 1546.119, 1552.313, 1558.507, 1564.702, 1570.896, \n",
    "            1577.09, 1583.285, 1589.479, 1595.673, 1601.868, 1608.062, \n",
    "            1614.256, 1620.451, 1626.645, 1632.839, 1639.034, 1645.228, \n",
    "            1651.423, 1657.617, 1663.811, 1670.006, 1676.2\n",
    "        ]\n",
    "        X_data = Detr.fit_transform(spc=X_data,wave=np.array(X_data.columns))\n",
    "    elif _normalization == 'derivate':\n",
    "        print(f\"Normalization DERIV{d} has been chosen!\")\n",
    "        X_data = NORM.fit_transform(spc=data, d=d, drop=True)\n",
    "    elif _normalization == \"Sav_Gol\":\n",
    "        print(f\"Normalization Sav_Gol{d} has been chosen!\")\n",
    "        X_data = NORM.transform(spc=data, window=11, poly=3, deriv=d)\n",
    "    elif _normalization == \"MinMax\":\n",
    "        print(\"Normalization MinMaxScaler has been chosen!\")\n",
    "        scaler = MinMaxScaler()  # x - x_min / x_max - x_min\n",
    "        X_data = scaler.fit_transform(data)\n",
    "    elif _normalization == \"StdScaler\":\n",
    "        print(\"Normalization StandardScaler has been chosen!\")\n",
    "        if _train:\n",
    "            X_data = NORM.fit_transform(data)\n",
    "        else:\n",
    "            X_data = NORM.transform(data)\n",
    "    elif _normalization == \"LOaO\":\n",
    "        print(\"Normalization Less One and One has been chosen!\")\n",
    "        X_data = LOaO(data)\n",
    "    elif _normalization is None:\n",
    "        X_data = data\n",
    "    else:\n",
    "        raise Exception(\"Sorry, not implemented yet!\")\n",
    "\n",
    "    return X_data, NORM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NIR-SC-UFES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "_data = \"PAD-UFES-IR\"\n",
    "Sampling_mode = \"None\"\n",
    "_augmentation = \"None\"\n",
    "_dataset_name = 'CandNC-ALL'  # Carcinoma_vs_ACK, Nev_vs_Mel\n",
    "_seed = 78645\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(f'data/IR-Spectroscopy-PAD-UFES-V4-{_dataset_name}.csv', decimal=\",\")\n",
    "\n",
    "# Drop rows where 'y' is 9 or 6\n",
    "dataset.drop(dataset[dataset['y'] == 9].index, inplace=True)\n",
    "dataset.drop(dataset[dataset['y'] == 6].index, inplace=True)\n",
    "dataset.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Extract labels and features\n",
    "labels = dataset.loc[:, ['y']]\n",
    "features = dataset.loc[:, 'data1':'data125']\n",
    "classes = dataset.loc[:, ['Classe']]\n",
    "\n",
    "# Concatenate features and classes\n",
    "FeaturesAndClass = pd.concat([features, classes], axis=1)\n",
    "\n",
    "# Encode class labels as categorical codes\n",
    "labels_subclasses = pd.Categorical(FeaturesAndClass['Classe']).codes\n",
    "\n",
    "# Split the dataset into training+validation and test sets\n",
    "x_to_train_valid, X_test, y_to_train_valid, y_test = train_test_split(\n",
    "    FeaturesAndClass, labels, stratify=labels_subclasses, test_size=0.1, random_state=_seed\n",
    ")\n",
    "\n",
    "# Encode class labels for the training+validation set\n",
    "labels_subclasses_train_valid = pd.Categorical(x_to_train_valid['Classe']).codes\n",
    "\n",
    "# Split the training+validation set into training and validation sets\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    x_to_train_valid, y_to_train_valid, stratify=labels_subclasses_train_valid, test_size=0.112, random_state=_seed\n",
    ")\n",
    "\n",
    "# Print class distributions for training, validation, and test sets\n",
    "print(f\"X_train Classe: \\n{X_train['Classe'].value_counts()}\")\n",
    "print(f\"X_valid Classe: \\n{X_valid['Classe'].value_counts()}\")\n",
    "print(f\"X_test Classe: \\n{X_test['Classe'].value_counts()}\")\n",
    "\n",
    "# Reset index for all sets\n",
    "X_train.reset_index(inplace=True, drop=True)\n",
    "y_train.reset_index(inplace=True, drop=True)\n",
    "X_valid.reset_index(inplace=True, drop=True)\n",
    "y_valid.reset_index(inplace=True, drop=True)\n",
    "X_test.reset_index(inplace=True, drop=True)\n",
    "y_test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Extract test classes\n",
    "test_classes = X_test.loc[:, 'Classe']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train['y'].value_counts(), y_valid['y'].value_counts(), y_test['y'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_plot_raw_data = False\n",
    "_tsne_plot = False\n",
    "_test_plot = False\n",
    "\n",
    "if _plot_raw_data:\n",
    "    from matplotlib import colors as mcolors\n",
    "\n",
    "    columns = np.array([908.1, 914.294, 920.489, 926.683, 932.877, 939.072, \n",
    "                945.266, 951.46, 957.655, 963.849, 970.044, 976.238, \n",
    "                982.432, 988.627, 994.821, 1001.015, 1007.21, 1013.404, \n",
    "                1019.598, 1025.793, 1031.987, 1038.181, 1044.376, 1050.57, \n",
    "                1056.764, 1062.959, 1069.153, 1075.348, 1081.542, 1087.736, \n",
    "                1093.931, 1100.125, 1106.319, 1112.514, 1118.708, 1124.902, \n",
    "                1131.097, 1137.291, 1143.485, 1149.68, 1155.874, 1162.069, \n",
    "                1168.263, 1174.457, 1180.652, 1186.846, 1193.04, 1199.235, \n",
    "                1205.429, 1211.623, 1217.818, 1224.012, 1230.206, 1236.401, \n",
    "                1242.595, 1248.789, 1254.984, 1261.178, 1267.373, 1273.567, \n",
    "                1279.761, 1285.956, 1292.15, 1298.344, 1304.539, 1310.733, \n",
    "                1316.927, 1323.122, 1329.316, 1335.51, 1341.705, 1347.899, \n",
    "                1354.094, 1360.288, 1366.482, 1372.677, 1378.871, 1385.065, \n",
    "                1391.26, 1397.454, 1403.648, 1409.843, 1416.037, 1422.231, \n",
    "                1428.426, 1434.62, 1440.814, 1447.009, 1453.203, 1459.398, \n",
    "                1465.592, 1471.786, 1477.981, 1484.175, 1490.369, 1496.564, \n",
    "                1502.758, 1508.952, 1515.147, 1521.341, 1527.535, 1533.73, \n",
    "                1539.924, 1546.119, 1552.313, 1558.507, 1564.702, 1570.896, \n",
    "                1577.09, 1583.285, 1589.479, 1595.673, 1601.868, 1608.062, \n",
    "                1614.256, 1620.451, 1626.645, 1632.839, 1639.034, 1645.228, \n",
    "                1651.423, 1657.617, 1663.811, 1670.006, 1676.2])\n",
    "    \n",
    "    CBC_data = X_train[X_train['Classe'] == 'CBC'].reset_index(drop=True).drop('Classe', axis=1)\n",
    "    CEC_data = X_train[X_train['Classe'] == 'CEC'].reset_index(drop=True).drop('Classe', axis=1)\n",
    "    MEL_data = X_train[X_train['Classe'] == 'MEL'].reset_index(drop=True).drop('Classe', axis=1)\n",
    "    ACK_data = X_train[X_train['Classe'] == 'ACK'].reset_index(drop=True).drop('Classe', axis=1)\n",
    "    SEK_data = X_train[X_train['Classe'] == 'SEK'].reset_index(drop=True).drop('Classe', axis=1)\n",
    "    NEV_data = X_train[X_train['Classe'] == 'NEV'].reset_index(drop=True).drop('Classe', axis=1)\n",
    "\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.rcParams['legend.fontsize'] = 22\n",
    "    plt.rcParams.update({'font.size': 25})\n",
    "\n",
    "    plt.plot(columns, CBC_data.to_numpy()[6], linewidth=3, label=\"CBC\", color='#F2071B')\n",
    "    plt.plot(columns, CEC_data.to_numpy()[6], linewidth=3, label=\"CEC\", color='#FC5A50')\n",
    "    plt.plot(columns, MEL_data.to_numpy()[6], linewidth=3, label=\"MEL\", color='#000000')\n",
    "    plt.plot(columns, ACK_data.to_numpy()[6], linewidth=3, label=\"ACK\", color='#15B01A')\n",
    "    plt.plot(columns, SEK_data.to_numpy()[6], linewidth=3, label=\"SEK\", color='#7BC8F6')\n",
    "    plt.plot(columns, NEV_data.to_numpy()[6], linewidth=3, label=\"NEV\", color='#031CA6')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Comprimento de onda (nm)\")\n",
    "    plt.ylabel(\"Absorbância\")\n",
    "\n",
    "\n",
    "    dir_save = './plots_imgs/NIR-SC-UFES_6CLASS_OriginalData_Disserta.pdf'\n",
    "    plt.savefig(dir_save)\n",
    "    plt.show()\n",
    "\n",
    "    if _tsne_plot = :\n",
    "        data = dataset.loc[:,'data1':'data125']\n",
    "        data_labels = dataset.loc[:,['y']].squeeze(1)\n",
    "\n",
    "        _preprocess = True\n",
    "\n",
    "        if _preprocess:\n",
    "            print(\"Using preprocess data\")\n",
    "            features_norm, _ = preprocessing(data, snv(), _normalization = \"SNV\")\n",
    "        else:\n",
    "            print(\"Using original data\")\n",
    "            pass\n",
    "\n",
    "        # Sklearn\n",
    "        from sklearn.manifold import TSNE\n",
    "        # import fitsne\n",
    "\n",
    "        def plot_tsne(data, labels, n_components=2, perplexity=30, n_iter=750):\n",
    "            \"\"\"\n",
    "            Plot t-SNE data.\n",
    "\n",
    "            Args:\n",
    "                data: The data to be plotted.\n",
    "                labels: The labels of the data.\n",
    "                n_components: The number of dimensions to reduce to.\n",
    "                perplexity: The perplexity parameter.\n",
    "                n_iter: The number of iterations.\n",
    "\n",
    "            Returns:\n",
    "                A matplotlib figure object.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                tsne = TSNE(n_components=n_components, perplexity=perplexity, n_iter=n_iter, learning_rate = 'auto', init = 'pca')\n",
    "                df_x_embedded = tsne.fit_transform(X=data)\n",
    "                # df_x_embedded = fitsne.FItSNE(X=data, no_dims=n_components, perplexity=perplexity, max_iter=n_iter, rand_seed=_seed)\n",
    "\n",
    "                fig = plt.figure(figsize=(16,10))\n",
    "                ax = sns.scatterplot(x=df_x_embedded[:, 0], y=df_x_embedded[:, 1], \n",
    "                                hue=labels, legend='full', s=250, palette=sns.color_palette(\"bright\", len(set(labels))))\n",
    "                plt.title(f't-SNE Plot (Perplexity={perplexity}, Iterations={n_iter})')\n",
    "                handles, labels  =  ax.get_legend_handles_labels()\n",
    "                ax.legend(handles, [\"Benigno\", \"Maligno\"], loc='lower right')\n",
    "                return fig\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during t-SNE: {e}\")\n",
    "                return None\n",
    "\n",
    "        if _test_plot:\n",
    "            for i in range(5, 101, 5):\n",
    "            # i = 30\n",
    "                if _preprocess:\n",
    "                    fig = plot_tsne(features_norm, data_labels, n_components=2, perplexity=i, n_iter=1000)\n",
    "                else: \n",
    "                    fig = plot_tsne(data, data_labels, n_components=2, perplexity=i, n_iter=1000)\n",
    "                # plt.show()\n",
    "\n",
    "                if fig is not None:\n",
    "                    if _preprocess:\n",
    "                        plt.savefig(f'./t-SNE/NIR-SC-UFES/{i}_NIR-SC-UFES_fig_NORM.png')\n",
    "                    else:\n",
    "                        plt.savefig(f'./t-SNE/NIR-SC-UFES/{i}_NIR-SC-UFES_fig.png')\n",
    "                    plt.show()\n",
    "        else:\n",
    "            plt.figure(figsize=(16,10))\n",
    "            plt.rcParams['legend.fontsize'] = 22\n",
    "            plt.rcParams.update({'font.size': 25})\n",
    "\n",
    "            i = 30\n",
    "\n",
    "            if _preprocess:\n",
    "                print(\"pre-processed ON\")\n",
    "                fig = plot_tsne(features_norm, data_labels, n_components=2, perplexity=i, n_iter=1000)\n",
    "            else:\n",
    "                print(\"pre-processed OFF\")\n",
    "                fig = plot_tsne(data, data_labels, n_components=2, perplexity=i, n_iter=1000)\n",
    "            \n",
    "            if fig is not None:\n",
    "                if _preprocess:\n",
    "                    plt.savefig(f'./t-SNE/{i}_NIR-SC-UFES_fig_NORM.pdf')\n",
    "                    plt.savefig(f'./t-SNE/{i}_NIR-SC-UFES_fig_NORM.png')\n",
    "                else:\n",
    "                    plt.savefig(f'./t-SNE/{i}_NIR-SC-UFES_fig.pdf')\n",
    "                    plt.savefig(f'./t-SNE/{i}_NIR-SC-UFES_fig.png')\n",
    "                plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE 1-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loader(data, target, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    Convert data and target to PyTorch tensors and prepare a data loader.\n",
    "\n",
    "    Parameters:\n",
    "    data (pd.DataFrame or np.ndarray): Input data to be converted to tensors.\n",
    "    target (pd.Series or np.ndarray): Target data to be converted to tensors.\n",
    "    batch_size (int, optional): Batch size for the data loader. Defaults to 32.\n",
    "    shuffle (bool, optional): Whether to shuffle the data. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the data loader and a dictionary with the sizes of the tensors.\n",
    "    \"\"\"\n",
    "    # Convert data and target to numpy arrays and then to PyTorch tensors\n",
    "    tensor_data = torch.tensor(np.array(data, dtype=np.float32))\n",
    "    tensor_target = torch.tensor(np.array(target, dtype=np.float32))\n",
    "\n",
    "    # Create a TensorDataset from the data and target tensors\n",
    "    dataset = torch.utils.data.TensorDataset(tensor_data, tensor_target)\n",
    "\n",
    "    # Create a DataLoader from the dataset\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    # Create a dictionary to store the sizes of the data and target tensors\n",
    "    size = {\n",
    "        'x_size': tensor_data.size(),\n",
    "        'y_size': tensor_target.size()\n",
    "    }\n",
    "\n",
    "    return dataloader, size\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE - IR - Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'E_MLP-IR_CNN-1D'\n",
    "conv = True\n",
    "    \n",
    "class ResNet1DBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A 1D Residual Block used in ResNet architecture.\n",
    "\n",
    "    Parameters:\n",
    "    in_channels (int): Number of input channels.\n",
    "    out_channels (int): Number of output channels.\n",
    "    kernel_size (int, optional): Size of the convolving kernel. Defaults to 3.\n",
    "    stride (int, optional): Stride of the convolution. Defaults to 1.\n",
    "    padding (int, optional): Zero-padding added to both sides of the input. Defaults to 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ResNet1DBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.shortcut = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the ResNet1DBlock.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor after applying the ResNet block.\n",
    "        \"\"\"\n",
    "        out = self.conv1(x)\n",
    "        out = self.tanh(out)\n",
    "        out = self.conv2(out)\n",
    "        shortcut = self.shortcut(x)\n",
    "        out += shortcut\n",
    "        out = self.tanh(out)\n",
    "        return out\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    A Variational Autoencoder (VAE) with 1D ResNet blocks.\n",
    "\n",
    "    Parameters:\n",
    "    latent_dims (int): Dimension of the latent space.\n",
    "    attention (optional): Attention mechanism. Defaults to None.\n",
    "    incerteza (bool, optional): Flag for uncertainty. Defaults to False.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dims, attention = None, incerteza=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = attention\n",
    "        self.incerteza = incerteza\n",
    "\n",
    "        # Define the encoder\n",
    "        self.encoder = nn.Sequential(OrderedDict([\n",
    "            ('ConvUnit_1', nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)),\n",
    "            ('ConvUnit_2', ResNet1DBlock(32, 64)),\n",
    "            ('ConvUnit_3', ResNet1DBlock(64, 128)),\n",
    "        ]))\n",
    "\n",
    "        # Determine the size of the flattened layer\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 1, 125) # NIR-SC-UFES\n",
    "            # dummy_input = torch.randn(1, 1, 1401) # BCA\n",
    "            # dummy_input = torch.randn(1, 1, 22) # BCA w/ PCA\n",
    "            # dummy_input = torch.randn(1, 1, 1557) # Urine\n",
    "            # dummy_input = torch.randn(1, 1, 1555) # Urine w/ derivative\n",
    "            \n",
    "            dummy_input = self.encoder(dummy_input)\n",
    "            self.flattened_size = dummy_input.view(dummy_input.size(0), -1).size(1)\n",
    "\n",
    "        # Define the mean and log-variance layers\n",
    "        self.mean_layer = nn.Linear(self.flattened_size, latent_dims)\n",
    "        self.logvar_layer = nn.Linear(self.flattened_size, latent_dims)\n",
    "\n",
    "        # Define the fully connected layer for the decoder\n",
    "        self.fc = nn.Linear(latent_dims, self.flattened_size)\n",
    "\n",
    "        # Define the decoder\n",
    "        self.decoder = nn.Sequential(OrderedDict([\n",
    "            ('DeLinearUnit_1', ResNet1DBlock(128, 64)),\n",
    "            ('DeLinearUnit_2', ResNet1DBlock(64, 32)),\n",
    "            ('DeLinearUnit_3', nn.Conv1d(32, 1, kernel_size=3, stride=1, padding=1)),\n",
    "        ]))\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode the input into the latent space.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Mean and log-variance of the encoded tensor.\n",
    "        \"\"\"\n",
    "        z = self.encoder(x)\n",
    "        z = torch.reshape(z, (-1, self.flattened_size))\n",
    "        mean = self.mean_layer(z)\n",
    "        logvar = self.logvar_layer(z)\n",
    "        return mean, logvar\n",
    "    \n",
    "    def decode(self, x):\n",
    "        \"\"\"\n",
    "        Decode the latent representation back to the input space.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Latent representation tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Reconstructed tensor.\n",
    "        \"\"\"\n",
    "        z = self.fc(x)\n",
    "        z = torch.reshape(z, (-1, 128, self.flattened_size // 128))\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def reparameterize_trick(self, mean, logvar):\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from the latent space.\n",
    "\n",
    "        Parameters:\n",
    "        mean (torch.Tensor): Mean of the latent distribution.\n",
    "        logvar (torch.Tensor): Log-variance of the latent distribution.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Sampled latent vector.\n",
    "        \"\"\"\n",
    "        std_dev = torch.exp(0.5 * logvar)\n",
    "        epsilon = torch.randn_like(std_dev)\n",
    "        return epsilon * std_dev + mean\n",
    "\n",
    "    def sample_latent_vector(self, x):\n",
    "        \"\"\"\n",
    "        Sample a latent vector from the input.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Sampled latent vector.\n",
    "        \"\"\"\n",
    "        mean, logvar = self.encode(x)\n",
    "        z = self.reparameterize_trick(mean, logvar)\n",
    "        return z\n",
    "        \n",
    "    def mean_std(self, x):\n",
    "        \"\"\"\n",
    "        Get the mean and standard deviation of the encoded input.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Mean and standard deviation tensors.\n",
    "        \"\"\"\n",
    "        mean, logvar = self.encode(x)\n",
    "        std_dev = torch.exp(0.5 * logvar)\n",
    "        return mean, std_dev\n",
    "\n",
    "    def forward(self, x, encoder=True, decoder=False):\n",
    "        \"\"\"\n",
    "        Forward pass for the VAE.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        encoder (bool, optional): Flag to return mean and std from encoder. Defaults to True.\n",
    "        decoder (bool, optional): Flag to return decoded output. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        tuple or torch.Tensor: Mean and standard deviation if encoder is True, otherwise decoded output.\n",
    "        \"\"\"\n",
    "        if encoder:\n",
    "            mean, logvar = self.encode(x)\n",
    "            std_dev = torch.exp(0.5 * logvar)\n",
    "            return mean, std_dev\n",
    "        elif decoder:\n",
    "            mean, logvar = self.encode(x)\n",
    "            z = self.reparameterize_trick(mean, logvar)\n",
    "            return self.decode(z), mean, logvar"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP Classify Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network for the classification tasks of VAE-GNA.\n",
    "\n",
    "    Parameters:\n",
    "    num_ftrs (int): Number of input features.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_ftrs):\n",
    "        super(ClassifyingNetwork, self).__init__()\n",
    "        \n",
    "        # Define a simple Multi-Layer Perceptron (MLP) for classification task\n",
    "        self.MLPclassify = nn.Sequential(\n",
    "            nn.Flatten(),                           # Flatten the input tensor\n",
    "            nn.Linear(num_ftrs, num_ftrs // 2),     # First fully connected layer\n",
    "            nn.BatchNorm1d(num_ftrs // 2),          # Batch normalization for regularization\n",
    "            nn.Dropout(0.1),                        # Dropout for regularization\n",
    "            nn.Tanh(),                              # Tanh activation function\n",
    "            nn.Linear(num_ftrs // 2, 2),            # Second fully connected layer to output 2 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the ClassifyingNetwork.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor with class scores.\n",
    "        \"\"\"\n",
    "        x = x.to(device)            # Move the input tensor to the appropriate device (CPU or GPU)\n",
    "        z = self.MLPclassify(x)     # Pass the input through the MLP\n",
    "        \n",
    "        return z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Attention Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention layer for VAE-GNA.\n",
    "\n",
    "    Parameters:\n",
    "    mean_std_size (int): Size of the input features (mean and std combined).\n",
    "    mean_size (int): Size of the mean features.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean_std_size, mean_size):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        # Define the attention mechanism\n",
    "        self.Attention = nn.Sequential(\n",
    "            nn.Linear(mean_std_size, mean_size),  # Fully connected layer to reduce dimension\n",
    "            nn.BatchNorm1d(mean_size),            # Batch normalization for regularization\n",
    "            nn.Sigmoid(),                         # Sigmoid activation function\n",
    "        )\n",
    "\n",
    "        # Define the softmax layer\n",
    "        self.Softlayer = nn.Sequential(\n",
    "            nn.Softmax(dim=1)  # Softmax activation function over the specified dimension\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for the AttentionLayer.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor after applying the attention mechanism and softmax.\n",
    "        \"\"\"\n",
    "        Z = self.Attention(x)  # Apply the attention mechanism\n",
    "        Z = self.Softlayer(Z)  # Apply softmax activation\n",
    "        return Z\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def optuna_run(_classify=False, optim_params=None):\n",
    "    \"\"\"\n",
    "    Run Optuna for hyperparameter optimization with cross-validation.\n",
    "\n",
    "    Parameters:\n",
    "    _classify (bool, optional): Flag indicating if the model is for classification. Defaults to False.\n",
    "    optim_params (dict, optional): Dictionary of fixed parameters for optimization. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    dict: Best hyperparameters found during optimization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters definition\n",
    "    _k_folds = 5\n",
    "    _lr = 1e-4 # Learning rate\n",
    "    _epochs = 2000\n",
    "    _sched_factor = 0.1 \n",
    "    _sched_min_lr = 1e-6\n",
    "    _sched_patience = 20\n",
    "    \n",
    "    # Normalization method\n",
    "    _normalization = \"SNV\" # \"SNV\", \"MinMax\", \"StdScaler\", \"LOaO\", \"SNV_Detrend\", \"derivate\", Sav_Gol\n",
    "    \n",
    "    # Loss function\n",
    "    _set_loss = \"cross_entropy_loss\" # \"adaptative_focal_loss\" or \"focal_loss\" or \"cross_entropy_loss\"\n",
    "\n",
    "    # For fold results\n",
    "    results = {}\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = StratifiedKFold(n_splits=_k_folds, random_state=_seed, shuffle=True)\n",
    "\n",
    "    # Start print\n",
    "    print('--------------------------------')\n",
    "\n",
    "    trials = []\n",
    "    \n",
    "    # Select Data Type\n",
    "    print(\"Normal has been choice!\")\n",
    "    X = pd.concat([X_train, X_valid])\n",
    "    y = pd.concat([y_train, y_valid])\n",
    "    \n",
    "    # Get subclass labels for training and validation data\n",
    "    try:\n",
    "        labels_subclasses_train_valid = pd.Categorical(X['Classe']).codes\n",
    "    except:\n",
    "        labels_subclasses_train_valid = y\n",
    "\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_index, valid_index) in enumerate(kfold.split(X, labels_subclasses_train_valid)):\n",
    "\n",
    "        # Print\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        # Obtém os dados de treinamento e validação para o fold atual\n",
    "        X_train_fold, X_valid_fold = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train_fold, y_valid_fold = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "        if Sampling_mode == 'None':\n",
    "            print(\"None Oversampling mode has been chosen!\")\n",
    "        else:\n",
    "            raise Exception(\"Sorry, not implemented yet!\")\n",
    "\n",
    "        try:\n",
    "            X_train_fold = X_train_fold.drop(['Classe'], axis=1)\n",
    "            X_valid_fold = X_valid_fold.drop(['Classe'], axis=1)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "        # Set normalization\n",
    "        NORM = snv() # None, derivative(), sav_gol(), StandardScaler()\n",
    "\n",
    "        # Preprocess the data\n",
    "        X_train_fold, NORM = preprocessing(X_train_fold, NORM, _normalization, _train=True)\n",
    "        X_valid_fold, _ = preprocessing(X_valid_fold, NORM, _normalization, _train=False)\n",
    "        \n",
    "        if conv:\n",
    "            X_train_fold = np.expand_dims(X_train_fold, 1)\n",
    "            X_valid_fold = np.expand_dims(X_valid_fold, 1)\n",
    "    \n",
    "        if np.isnan(X_train_fold).any():\n",
    "            print(\"Your tensor X_train_fold contains NaN values!\")      \n",
    "\n",
    "        MSE_criterion = nn.MSELoss(reduction='sum')\n",
    "        \n",
    "        prev_loss = np.inf\n",
    "        train_acc = 0.0\n",
    "        valid_acc = 0.0\n",
    "        limit_stop = 20 #100\n",
    "\n",
    "        x_train_ftrs = torch.Tensor([]).to(device)\n",
    "        y_train_ftrs = torch.Tensor([]).to(device)\n",
    "        x_valid_ftrs = torch.Tensor([]).to(device)\n",
    "        y_valid_ftrs = torch.Tensor([]).to(device)\n",
    "\n",
    "        max_latent_dims_parm_opt = X_train_fold.shape[2] // 2\n",
    "\n",
    "        def objective(trial):\n",
    "            \"\"\"\n",
    "            Objective function for Optuna.\n",
    "\n",
    "            Parameters:\n",
    "            trial (optuna.trial.Trial): A trial object for parameter suggestions.\n",
    "\n",
    "            Returns:\n",
    "            float: Metric to be optimized by Optuna.\n",
    "            \"\"\"\n",
    "            _gamma_1 = 1\n",
    "\n",
    "            if _classify:\n",
    "                _gamma_2 = optim_params['_gamma_2']\n",
    "                _latent_dims = optim_params['_latent_dims']\n",
    "                _batch_size = optim_params['_batch_size']\n",
    "                _gamma_3 = trial.suggest_float('_gamma_3', 0.001, 100)\n",
    "            else:\n",
    "                _gamma_2 = trial.suggest_float('_gamma_2', 0.001, 100)\n",
    "                _latent_dims = trial.suggest_int('_latent_dims', 4, 512)\n",
    "                _batch_size  = trial.suggest_int('_batch_size', 12, 256, 12)\n",
    "\n",
    "            train_dataloader, train_size = prepare_data_loader(X_train_fold, y_train_fold, batch_size=_batch_size, shuffle=True)\n",
    "            valid_dataloader, valid_size = prepare_data_loader(X_valid_fold, y_valid_fold, batch_size=_batch_size, shuffle=False)\n",
    "\n",
    "            if _incerteza:\n",
    "                input_attention_dims = _latent_dims\n",
    "            else:\n",
    "                input_attention_dims = _latent_dims*2\n",
    "\n",
    "            if _attention:\n",
    "                ATT_Layer = AttentionLayer(input_attention_dims, _latent_dims).to(device)\n",
    "            else:\n",
    "                ATT_Layer = None\n",
    "\n",
    "            VAE_network = VAE(latent_dims=_latent_dims).to(device)\n",
    "\n",
    "            if _classify:\n",
    "                CLASS_network = ClassifyingNetwork(num_ftrs=input_attention_dims).to(device)\n",
    "            else:\n",
    "                CLASS_network = None\n",
    "\n",
    "            model_path = './model/'\n",
    "            vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}.pth'\n",
    "            \n",
    "            if os.path.exists(vae_model_path) and _classify:\n",
    "                VAE_network.load_state_dict(torch.load(vae_model_path, map_location=device))\n",
    "\n",
    "            JOINT_Model = JointModel(VAE_network, CLASS_network, ATT_Layer)\n",
    "\n",
    "            opt_parameters = list(VAE_network.parameters())\n",
    "\n",
    "            if _classify:\n",
    "                opt_parameters += list(CLASS_network.parameters())\n",
    "            \n",
    "            if _attention:\n",
    "                opt_parameters += list(ATT_Layer.parameters())\n",
    "\n",
    "            optimizer = torch.optim.Adam(opt_parameters, weight_decay=1e-3, lr=_lr)\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=_sched_factor, min_lr=_sched_min_lr, patience=_sched_patience)\n",
    "\n",
    "            if _classify:\n",
    "                if _set_loss == \"focal_loss\":\n",
    "                    CLASS_criterion = FocalLoss(alpha=0.25, gamma=2.0, reduction='sum')\n",
    "                elif _set_loss == \"adaptative_focal_loss\":\n",
    "                    CLASS_criterion = AdaptativeFocalLoss(alpha=0.25, gamma=2.0, reduction='sum')\n",
    "                elif _set_loss == \"cross_entropy_loss\":\n",
    "                    CLASS_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "                else:\n",
    "                    raise NotImplementedError(f\"Invalid Loss: {_set_loss}\")\n",
    "\n",
    "            # Run NN\n",
    "            train_loss = []\n",
    "            valid_loss = []\n",
    "            running_loss = 0.0\n",
    "            running_kld = 0.0\n",
    "            running_recon = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for i, train_data in enumerate(train_dataloader):\n",
    "\n",
    "                inputs, labels = train_data\n",
    "                \n",
    "                if conv:\n",
    "                    inputs = inputs.to(device)\n",
    "                else:\n",
    "                    inputs = inputs.to(device).squeeze(1)\n",
    "\n",
    "                labels = labels.type(dtype=torch.LongTensor)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                try:\n",
    "                    labels = labels.squeeze(1)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Variational AutoEncoder NN\n",
    "                inputs_pred, mean, logvar = VAE_network.forward(inputs, encoder=False, decoder=True)\n",
    "            \n",
    "                MSE_loss = MSE_criterion(inputs_pred, inputs) # Reconstruction Loss \n",
    "                KLD_loss = - 0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp()) # KLD Loss \n",
    "                VAE_loss = (_gamma_1 * MSE_loss) + (_gamma_2 * KLD_loss)\n",
    "\n",
    "                if _classify:\n",
    "                    outputs = JOINT_Model(inputs, attention=_attention, incerteza=_incerteza)\n",
    "\n",
    "                    _, y_pred = torch.max(outputs, 1)\n",
    "                \n",
    "                    if _set_loss == \"adaptative_focal_loss\":\n",
    "                        CLASS_loss = CLASS_criterion(outputs, labels, torch.Tensor([1]))\n",
    "                        beta += 1\n",
    "                    else:\n",
    "                        CLASS_loss = CLASS_criterion(outputs, labels)\n",
    "                    \n",
    "                    running_corrects += torch.sum(y_pred == labels.data)\n",
    "                    t_loss = VAE_loss + (_gamma_3 * CLASS_loss)\n",
    "                else:\n",
    "                    t_loss =  VAE_loss\n",
    "\n",
    "                \n",
    "                running_recon += _gamma_1 * MSE_loss.item() * inputs.size(0)\n",
    "                running_kld += _gamma_2 * KLD_loss.item() * inputs.size(0)\n",
    "                running_loss += t_loss.item() * inputs.size(0)\n",
    "\n",
    "                t_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if _classify:\n",
    "                train_acc = running_corrects.double() / train_size['y_size'][0]\n",
    "            \n",
    "\n",
    "            epoch_kld = running_kld / train_size['y_size'][0]\n",
    "            epoch_recon = running_recon / train_size['y_size'][0]\n",
    "            epoch_loss = running_loss / train_size['y_size'][0]\n",
    "            # train_loss_graph.append(epoch_loss)\n",
    "\n",
    "            # Validation\n",
    "            with torch.no_grad():\n",
    "                ypredVector = []\n",
    "                labelsVector = []\n",
    "\n",
    "                valid_running_loss = 0.0\n",
    "                valid_running_kld = 0.0\n",
    "                valid_running_recon = 0.0\n",
    "                valid_running_corrects = 0\n",
    "\n",
    "                for i, valid_data in enumerate(valid_dataloader):\n",
    "                    \n",
    "                    inputs, labels = valid_data\n",
    "                    \n",
    "                    if conv:\n",
    "                        inputs = inputs.to(device)\n",
    "                    else:\n",
    "                        inputs = inputs.to(device).squeeze(1)\n",
    "                    \n",
    "                    labels = labels.type(dtype=torch.LongTensor)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    try:\n",
    "                        labels = labels.squeeze(1)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    # Variational AutoEncoder NN\n",
    "                    x_pred, mean, logvar = VAE_network.forward(inputs, encoder=False, decoder=True)\n",
    "                \n",
    "                    MSE_loss = MSE_criterion(x_pred, inputs)\n",
    "\n",
    "                    KLD_loss = - 0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "                    TVAE_loss = (_gamma_1 * MSE_loss) + (_gamma_2 * KLD_loss)\n",
    "\n",
    "                    if _classify:\n",
    "                        outputs = JOINT_Model(inputs, attention=_attention, incerteza=_incerteza)\n",
    "                            \n",
    "                        _, y_pred = torch.max(outputs, 1)\n",
    "\n",
    "                        if _set_loss == \"adaptative_focal_loss\":\n",
    "                            CLASS_loss = CLASS_criterion(outputs, labels, beta)\n",
    "                        else:\n",
    "                            CLASS_loss = CLASS_criterion(outputs, labels)\n",
    "\n",
    "                        valid_running_corrects += torch.sum(y_pred == labels.data)\n",
    "                        loss = TVAE_loss + (_gamma_3 * CLASS_loss)\n",
    "                        ypredVector += y_pred.detach().cpu()\n",
    "                        labelsVector += labels.data.detach().cpu()\n",
    "                    else:\n",
    "                        loss = TVAE_loss\n",
    "\n",
    "                    valid_running_loss += loss.item() * inputs.size(0)\n",
    "                    valid_running_recon += _gamma_1 * MSE_loss.item() * inputs.size(0)\n",
    "                    valid_running_kld += _gamma_2 * KLD_loss.item() * inputs.size(0)\n",
    "\n",
    "\n",
    "                valid_epoch_loss = valid_running_loss / valid_size['y_size'][0]\n",
    "                scheduler.step(valid_epoch_loss)\n",
    "                scheduler.get_last_lr()\n",
    "\n",
    "            if _classify:\n",
    "                valid_acc = valid_running_corrects.double() / valid_size['y_size'][0]\n",
    "                results[fold] = 100.0 * valid_acc\n",
    "\n",
    "                balancedAccuracyScore = balanced_accuracy_score(labelsVector, ypredVector)\n",
    "\n",
    "                return balancedAccuracyScore\n",
    "            else:\n",
    "                return valid_epoch_loss\n",
    "        \n",
    "        if _classify:\n",
    "            study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler())\n",
    "        else:\n",
    "            study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler())\n",
    "        \n",
    "        study.optimize(objective, n_trials=_epochs)\n",
    "        trial = study.best_trial\n",
    "        trials.append(trial)\n",
    "\n",
    "\n",
    "    if _classify:\n",
    "        MAX = -np.inf\n",
    "        for i, trial in enumerate(trials):\n",
    "            if trial.value > MAX:\n",
    "                best_value = trial.values\n",
    "                best_params = trial.params\n",
    "                best_fold = i\n",
    "                MAX = trial.value\n",
    "\n",
    "        print(f\"  Fold: {best_fold}\")\n",
    "        print(f\"  Value: {best_value}\")\n",
    "\n",
    "        print(\"  Params: \")\n",
    "        for key, value in best_params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "        \n",
    "        return best_params\n",
    "    else:\n",
    "        MIN = np.inf\n",
    "        for i, trial in enumerate(trials):\n",
    "            print(trial.value)\n",
    "            if trial.value < MIN:\n",
    "                best_value = trial.value\n",
    "                best_params = trial.params\n",
    "                best_fold = i\n",
    "                MIN = trial.value\n",
    "\n",
    "        print(f\"  Fold: {best_fold}\")\n",
    "        print(f\"  Value: {best_value}\")\n",
    "\n",
    "        print(\"  Params: \")\n",
    "        for key, value in best_params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "\n",
    "        return best_params\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Hyperparameters definition\n",
    "_k_folds = 5  # Number of folds for K-fold cross-validation\n",
    "_lr = 1e-4  # Learning rate\n",
    "_epochs = 2000  # Number of training epochs\n",
    "_sched_factor = 0.1  # Factor by which the learning rate is reduced\n",
    "_sched_min_lr = 1e-6  # Minimum learning rate for the scheduler\n",
    "_sched_patience = 20  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "\n",
    "# Normalization method\n",
    "_normalization = \"SNV\" # Options: (\"SNV\", \"MinMax\", \"StdScaler\", \"LOaO\", \"SNV_Detrend\", \"derivate\", \"Sav_Gol\")\n",
    "\n",
    "# Loss function\n",
    "_set_loss = \"cross_entropy_loss\" # Loss function to be used (\"cross_entropy_loss\", \"adaptative_focal_loss\", \"focal_loss\")\n",
    "\n",
    "# Loss weight for reconstruction loss in the VAE\n",
    "_gamma_1 = 1\n",
    "\n",
    "_attention = False\n",
    "_incerteza = False\n",
    "_att_method = 'Loung'  # Attention method\n",
    "_plotLoss = True  # Flag to plot loss\n",
    "\n",
    "class JointModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A joint model combining a Variational Autoencoder (VAE), a Classifying Network, and an Attention Layer.\n",
    "\n",
    "    Parameters:\n",
    "    VAE (nn.Module): The Variational Autoencoder model.\n",
    "    ClassifyingNetwork (nn.Module): The Classifying Network model.\n",
    "    AttentionLayer (nn.Module): The Attention Layer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, VAE, ClassifyingNetwork, AttentionLayer):\n",
    "        super(JointModel, self).__init__()\n",
    "        self.vae = VAE\n",
    "        if ClassifyingNetwork is not None:\n",
    "            self.classifying_net = ClassifyingNetwork\n",
    "        if AttentionLayer is not None:\n",
    "            self.attention_layer = AttentionLayer\n",
    "        \n",
    "    def forward(self, x, attention=False, incerteza=False, method='Loung'):\n",
    "        \"\"\"\n",
    "        Forward pass for the JointModel.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        attention (bool, optional): Flag to apply attention. Defaults to False.\n",
    "        incerteza (bool, optional): Flag to handle uncertainty. Defaults to False.\n",
    "        method (str, optional): Method for attention mechanism. Defaults to 'Loung'.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor after passing through the joint model.\n",
    "        \"\"\"\n",
    "        # Encode the input using the VAE to get mean and std deviation\n",
    "        mean_vae, std_dev = self.vae.forward(x)\n",
    "        ms_vector = torch.cat((mean_vae, std_dev), dim=1)\n",
    "\n",
    "        if incerteza:\n",
    "            # Handling uncertainty\n",
    "            weight_mean_nn = self.classifying_net.MLPclassify[1].weight\n",
    "            weight_std_nn = self.classifying_net.MLPclassify[1].weight\n",
    "            bias = self.classifying_net.MLPclassify[1].bias\n",
    "            \n",
    "            mean_vae_new = mean_vae.unsqueeze(2)\n",
    "            std_dev_new = std_dev.unsqueeze(2)\n",
    "\n",
    "            z_mean = torch.zeros_like(mean_vae_new)\n",
    "            z_std = torch.zeros_like(std_dev_new)\n",
    "\n",
    "            # Expand weights mean, std and biases using linear interpolation\n",
    "            weight_mean = torch.nn.functional.interpolate(\n",
    "                weight_mean_nn.unsqueeze(0).unsqueeze(0),\n",
    "                size=(mean_vae_new.size()[1], mean_vae_new.size()[1]),\n",
    "                mode='bilinear',\n",
    "                align_corners=False,\n",
    "            ).squeeze(0).squeeze(0)\n",
    "            \n",
    "            weight_std = torch.nn.functional.interpolate(\n",
    "                weight_std_nn.unsqueeze(0).unsqueeze(0),\n",
    "                size=(std_dev_new.size()[1], std_dev_new.size()[1]),\n",
    "                mode='bilinear',\n",
    "                align_corners=False,\n",
    "            ).squeeze(0).squeeze(0)\n",
    "            \n",
    "            bias = torch.linspace(bias.min().item(), bias.max().item(), steps=mean_vae_new.size()[1]).view(-1, 1).to(device)\n",
    "\n",
    "            # Increase the vector size to accommodate the repetition.\n",
    "            weight_mean = weight_mean.unsqueeze(0)\n",
    "            weight_std = weight_std.unsqueeze(0)\n",
    "            bias = bias.unsqueeze(0)\n",
    "\n",
    "            # Replicate the vector along axis 0.\n",
    "            repeated_vector_mean = weight_mean.repeat(mean_vae_new.size()[0], 1, 1)\n",
    "            repeated_vector_std = weight_std.repeat(std_dev_new.size()[0], 1, 1)\n",
    "            repeated_bias = bias.repeat(mean_vae_new.size()[0], 1, 1)\n",
    "\n",
    "            # Remove the additional dimension\n",
    "            weight_mean = repeated_vector_mean.squeeze(0)\n",
    "            weight_std = repeated_vector_std.squeeze(0)\n",
    "            bias = repeated_bias.squeeze(0)\n",
    "\n",
    "            for i in range(mean_vae_new.size()[0]): # Referente aos batchs\n",
    "                z_mean[i] = torch.matmul(weight_mean[i], mean_vae_new[i]) + bias[i]\n",
    "\n",
    "            for i in range(std_dev_new.size()[0]):\n",
    "                z_std[i] = torch.matmul(weight_std[i]**2, std_dev_new[i]**2)\n",
    "            \n",
    "        \n",
    "            z_mean = z_mean.squeeze(2)\n",
    "            z_std = z_std.squeeze(2)\n",
    "            \n",
    "            ms_vector = expected_sigm_of_norm(z_mean, z_std, method='probit')\n",
    "\n",
    "            if attention:\n",
    "                # Apply attention mechanism\n",
    "                attention_weights = self.attention_layer(ms_vector)\n",
    "                \n",
    "                ms_vector_new = ms_vector.clone()\n",
    "\n",
    "                if method == 'Loung':\n",
    "                    ms_vector_new *= attention_weights\n",
    "                elif method == 'Bahdanau':\n",
    "                    ms_vector_new += attention_weights\n",
    "\n",
    "                ms_vector = ms_vector_new\n",
    "        \n",
    "        # Pass the combined vector through the classifying network\n",
    "        output = self.classifying_net(ms_vector)\n",
    "        return output\n",
    "\n",
    "# Run optimization for both classification and non-classification cases\n",
    "for _classify in [False, True]:\n",
    "    \n",
    "    if _classify:\n",
    "        # Run Optuna optimization for classification and get the best parameters\n",
    "        best_params = optuna_run(_classify=True, optim_params=best_params)\n",
    "        _gamma_3 = best_params['_gamma_3']\n",
    "    else: \n",
    "        # Run Optuna optimization for non-classification and get the best parameters\n",
    "        best_params = optuna_run(_classify=False)\n",
    "        _gamma_2 = best_params['_gamma_2']  # Retrieve the best gamma_2 parameter\n",
    "        _latent_dims = best_params['_latent_dims']  # Retrieve the best latent dimensions\n",
    "        _batch_size = best_params['_batch_size']  # Retrieve the best batch size\n",
    "\n",
    "    # For storing fold results\n",
    "    results = {}\n",
    "\n",
    "    # Define the K-fold Cross Validator\n",
    "    kfold = StratifiedKFold(n_splits=_k_folds, random_state=_seed, shuffle=True)\n",
    "\n",
    "    # Start print\n",
    "    print('--------------------------------')\n",
    "\n",
    "\n",
    "    print(\"Normal has been choice!\")\n",
    "    X = pd.concat([X_train, X_valid])\n",
    "    y = pd.concat([y_train, y_valid])\n",
    "\n",
    "    # Get subclass labels for training and validation data\n",
    "    try:\n",
    "        labels_subclasses_train_valid = pd.Categorical(X['Classe']).codes\n",
    "    except:\n",
    "        labels_subclasses_train_valid = y\n",
    "\n",
    "    # Enable anomaly detection\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # K-fold Cross Validation model evaluation\n",
    "    for fold, (train_index, valid_index) in enumerate(kfold.split(X, labels_subclasses_train_valid)):\n",
    "\n",
    "        # Print the fold number\n",
    "        print(f'FOLD {fold}')\n",
    "        print('--------------------------------')\n",
    "\n",
    "        # Get the training and validation data for the current fold\n",
    "        X_train_fold, X_valid_fold = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train_fold, y_valid_fold = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "        # Handle oversampling based on the sampling mode\n",
    "        if Sampling_mode == 'SMOTE' or Sampling_mode == 'EMD_SMOTE':\n",
    "            print(\"Oversampling SMOTE has been choice!\")\n",
    "            from imblearn.over_sampling import SMOTE\n",
    "\n",
    "            try:\n",
    "                X_train_fold = X_train_fold.drop(['Classe'], axis=1)\n",
    "                X_valid_fold = X_valid_fold.drop(['Classe'], axis=1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            sm = SMOTE(random_state=_seed) # Only apply SMOTE to the training dataset\n",
    "            X_train_fold, y_train_fold = sm.fit_resample(X_train_fold, y_train_fold)\n",
    "        elif Sampling_mode == 'GAN':\n",
    "            print(\"Oversampling GAN has been choice!\")\n",
    "            X_train_fold = pd.concat([X_train_fold, GANDataset.loc[:,:'Classe']])\n",
    "            df_new = pd.DataFrame({'y': np.ones(num_samples_aug, dtype=int)})\n",
    "            y_train_fold = pd.concat([y_train_fold, df_new], ignore_index=True)\n",
    "        elif Sampling_mode == 'None':\n",
    "            print(\"None Oversampling mode has been chosen!\")\n",
    "        else:\n",
    "            raise Exception(\"Sorry, not implemented yet!\")\n",
    "\n",
    "        try:\n",
    "            X_train_fold = X_train_fold.drop(['Classe'], axis=1)\n",
    "            X_valid_fold = X_valid_fold.drop(['Classe'], axis=1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Set normalization method\n",
    "        NORM = snv()  # Options: None, derivative(), sav_gol(), StandardScaler()\n",
    "        \n",
    "        # Preprocess the data\n",
    "        X_train_fold, NORM = preprocessing(X_train_fold, NORM, _normalization, _train=True)\n",
    "        X_valid_fold, _ = preprocessing(X_valid_fold, NORM, _normalization, _train=False)\n",
    "\n",
    "        if conv:\n",
    "            # Expand dimensions for convolutional input\n",
    "            X_train_fold = np.expand_dims(X_train_fold, 1)\n",
    "            X_valid_fold = np.expand_dims(X_valid_fold, 1)\n",
    "\n",
    "         # Prepare data loaders\n",
    "        train_dataloader, train_size = prepare_data_loader(X_train_fold, y_train_fold, batch_size=_batch_size, shuffle=True)\n",
    "        valid_dataloader, valid_size = prepare_data_loader(X_valid_fold, y_valid_fold, batch_size=_batch_size, shuffle=False)\n",
    "            \n",
    "        # Set input dimensions for attention layer based on uncertainty handling\n",
    "        if _incerteza:\n",
    "            input_attention_dims = _latent_dims\n",
    "        else:\n",
    "            input_attention_dims = _latent_dims * 2\n",
    "\n",
    "        # Initialize attention layer if needed\n",
    "        if _attention:\n",
    "            ATT_Layer = AttentionLayer(input_attention_dims, _latent_dims).to(device)\n",
    "        else:\n",
    "            ATT_Layer = None\n",
    "\n",
    "         # Initialize the VAE network\n",
    "        VAE_network = VAE(latent_dims=_latent_dims).to(device)\n",
    "\n",
    "        # Initialize the classification network if classification is enabled\n",
    "        if _classify:\n",
    "            CLASS_network = ClassifyingNetwork(num_ftrs=input_attention_dims).to(device)\n",
    "        else:\n",
    "            CLASS_network = None\n",
    "\n",
    "        # Define model paths for saving/loading\n",
    "        model_path = './model/'\n",
    "        vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}.pth'\n",
    "        \n",
    "        # Load pre-trained VAE model if it exists\n",
    "        if os.path.exists(vae_model_path) and _classify:\n",
    "            print(f'VAE already exists. Loading model {vae_model_path}.')\n",
    "            VAE_network.load_state_dict(torch.load(vae_model_path, map_location=device))\n",
    "\n",
    "        # Initialize the joint model\n",
    "        JOINT_Model = JointModel(VAE_network, CLASS_network, ATT_Layer)\n",
    "\n",
    "        # Collect parameters for optimization\n",
    "        opt_parameters = list(VAE_network.parameters())\n",
    "\n",
    "        if _classify:\n",
    "            opt_parameters += list(CLASS_network.parameters())\n",
    "        \n",
    "        if _attention:\n",
    "            opt_parameters += list(ATT_Layer.parameters())\n",
    "\n",
    "        # Set optimizer and scheduler\n",
    "        optimizer = torch.optim.Adam(opt_parameters, weight_decay=1e-3, lr=_lr)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=_sched_factor, min_lr=_sched_min_lr, patience=_sched_patience, verbose=True)\n",
    "\n",
    "        # Set loss criteria\n",
    "        MSE_criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "        if _classify:\n",
    "            if _set_loss == \"focal_loss\":\n",
    "                print('Focal Loss Chosen!')\n",
    "                CLASS_criterion = FocalLoss(alpha=0.25, gamma=2.0, reduction='sum')\n",
    "            elif _set_loss == \"adaptative_focal_loss\":\n",
    "                print('Adaptative Focal Loss Chosen!')\n",
    "                CLASS_criterion = AdaptativeFocalLoss(alpha=0.25, gamma=2.0, reduction='sum')\n",
    "            elif _set_loss == \"cross_entropy_loss\":\n",
    "                print('Cross Entropy Loss Chosen!')\n",
    "                CLASS_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Invalid Loss: {_set_loss}\")\n",
    "        \n",
    "        # Train the model\n",
    "        print('Training VAE')\n",
    "        mean_train_loss = []\n",
    "        mean_valid_loss = []\n",
    "        curr_loss = 0\n",
    "        prev_loss = np.inf  # Initialize previous loss to infinity for comparison\n",
    "        train_acc = 0.0\n",
    "        valid_acc = 0.0\n",
    "        limit_stop = 20  # Early stopping limit\n",
    "        train_loss_graph = []\n",
    "        valid_loss_graph = []\n",
    "        train_acc_graph = []\n",
    "        valid_acc_graph = []\n",
    "\n",
    "        # Initialize tensors to store features and labels for training and validation\n",
    "        x_train_ftrs = torch.Tensor([]).to(device)\n",
    "        y_train_ftrs = torch.Tensor([]).to(device)\n",
    "        x_valid_ftrs = torch.Tensor([]).to(device)\n",
    "        y_valid_ftrs = torch.Tensor([]).to(device)\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(_epochs):\n",
    "            train_loss = []\n",
    "            valid_loss = []\n",
    "            running_loss = 0.0\n",
    "            running_kld = 0.0\n",
    "            running_recon = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Beta for Adaptative Focal Loss\n",
    "            beta = torch.Tensor([epoch+1])\n",
    "\n",
    "            # Iterate over training data\n",
    "            for i, train_data in enumerate(train_dataloader):\n",
    "\n",
    "                inputs, labels = train_data\n",
    "                \n",
    "                if conv:\n",
    "                    inputs = inputs.to(device)\n",
    "                else:\n",
    "                    inputs = inputs.to(device).squeeze(1)\n",
    "\n",
    "                labels = labels.type(dtype=torch.LongTensor)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                try:\n",
    "                    labels = labels.squeeze(1)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Variational AutoEncoder NN\n",
    "                inputs_pred, mean, logvar = VAE_network.forward(inputs, encoder=False, decoder=True)\n",
    "                \n",
    "                # Compute reconstruction loss (MSE) and KL divergence\n",
    "                MSE_loss = MSE_criterion(inputs_pred, inputs)\n",
    "                KLD_loss = - 0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "                VAE_loss = (_gamma_1 * MSE_loss) + (_gamma_2 * KLD_loss)\n",
    "                \n",
    "                if _classify:\n",
    "                    # Forward pass through joint model\n",
    "                    outputs = JOINT_Model(inputs, attention=_attention, incerteza=_incerteza, method=_att_method)\n",
    "                    \n",
    "                    _, y_pred = torch.max(outputs, 1)\n",
    "\n",
    "                    if _set_loss == \"adaptative_focal_loss\":\n",
    "                        CLASS_loss = CLASS_criterion(outputs, labels, beta)\n",
    "                        beta += 1\n",
    "                    else:\n",
    "                        CLASS_loss = CLASS_criterion(outputs, labels)\n",
    "\n",
    "                    running_corrects += torch.sum(y_pred == labels.data)\n",
    "                    t_loss = VAE_loss + (_gamma_3 * CLASS_loss)\n",
    "                else:\n",
    "                    t_loss =  VAE_loss\n",
    "\n",
    "                # Accumulate losses\n",
    "                running_recon += _gamma_1 * MSE_loss.item() * inputs.size(0)\n",
    "                running_kld += _gamma_2 * KLD_loss.item() * inputs.size(0)\n",
    "                running_loss += t_loss.item() * inputs.size(0)\n",
    "\n",
    "                t_loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "            if _classify:\n",
    "                train_acc = running_corrects.double() / train_size['y_size'][0]\n",
    "                train_acc_graph.append(train_acc)\n",
    "\n",
    "            epoch_kld = running_kld / train_size['y_size'][0]\n",
    "            epoch_recon = running_recon / train_size['y_size'][0]\n",
    "            epoch_loss = running_loss / train_size['y_size'][0]\n",
    "            train_loss_graph.append(epoch_loss)\n",
    "\n",
    "            # Validation step\n",
    "            with torch.no_grad():\n",
    "                valid_running_loss = 0.0\n",
    "                valid_running_kld = 0.0\n",
    "                valid_running_recon = 0.0\n",
    "                valid_running_corrects = 0\n",
    "\n",
    "                for i, valid_data in enumerate(valid_dataloader):\n",
    "\n",
    "                    inputs, labels = valid_data\n",
    "                    \n",
    "                    if conv:\n",
    "                        inputs = inputs.to(device)\n",
    "                    else:\n",
    "                        inputs = inputs.to(device).squeeze(1)\n",
    "                    \n",
    "                    labels = labels.type(dtype=torch.LongTensor)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    try:\n",
    "                        labels = labels.squeeze(1)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "\n",
    "                    # Variational AutoEncoder NN\n",
    "                    x_pred, mean, logvar = VAE_network.forward(inputs, encoder=False, decoder=True)\n",
    "\n",
    "                    MSE_loss = MSE_criterion(x_pred, inputs)\n",
    "                    KLD_loss = - 0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "                    TVAE_loss = (_gamma_1 * MSE_loss) + (_gamma_2 * KLD_loss)\n",
    "\n",
    "                    if _classify:\n",
    "                        outputs = JOINT_Model(inputs, attention=_attention, incerteza=_incerteza, method=_att_method)\n",
    "                            \n",
    "                        _, y_pred = torch.max(outputs, 1)\n",
    "                        \n",
    "                        if _set_loss == \"adaptative_focal_loss\":\n",
    "                            CLASS_loss = CLASS_criterion(outputs, labels, beta)\n",
    "                        else:\n",
    "                            CLASS_loss = CLASS_criterion(outputs, labels)\n",
    "\n",
    "                        valid_running_corrects += torch.sum(y_pred == labels.data)\n",
    "                        loss = TVAE_loss + (_gamma_3 * CLASS_loss)\n",
    "                    else:\n",
    "                        loss = TVAE_loss\n",
    "\n",
    "                    valid_running_loss += loss.item() * inputs.size(0)\n",
    "                    valid_running_recon += _gamma_1 * MSE_loss.item() * inputs.size(0)\n",
    "                    valid_running_kld += _gamma_2 * KLD_loss.item() * inputs.size(0)\n",
    "\n",
    "                if epoch % 10 == 0 and not _classify:\n",
    "                    plot_gallery([inputs.detach().cpu(), x_pred.detach().cpu()], epoch, fold, model_name, 1, 2, all_plot=True)  \n",
    "\n",
    "                valid_epoch_loss = valid_running_loss / valid_size['y_size'][0]\n",
    "                valid_epoch_kld = valid_running_kld / valid_size['y_size'][0]\n",
    "                valid_epoch_recon = valid_running_recon / valid_size['y_size'][0]\n",
    "                valid_loss_graph.append(valid_epoch_loss)\n",
    "\n",
    "                scheduler.step(valid_epoch_loss)\n",
    "                scheduler.get_last_lr()\n",
    "                curr_loss = valid_epoch_loss\n",
    "\n",
    "            if _classify:\n",
    "                valid_acc = valid_running_corrects.double() / valid_size['y_size'][0]\n",
    "                valid_acc_graph.append(valid_acc)\n",
    "                \n",
    "                results[fold] = 100.0 * valid_acc\n",
    "                print(f'Epoch: {epoch}')\n",
    "                print(f'Train loss: {epoch_loss}, Train Acc: {train_acc*100:.2f}')\n",
    "                print(f'Valid loss: {valid_epoch_loss}, Valid  Acc: {valid_acc*100:.2f}')\n",
    "            else:\n",
    "                print(f'Epoch: {epoch} | Train loss: {epoch_loss} | Train KLD: {epoch_kld} | Train Recon Loss:{epoch_recon}')\n",
    "                print(f'Epoch: {epoch} | Valid loss: {valid_epoch_loss} | Valid KLD: {valid_epoch_kld} | Valid Recon Loss:{valid_epoch_recon}')\n",
    "            \n",
    "            # Check if the current loss is less than the best so far\n",
    "            if curr_loss < prev_loss:\n",
    "                best_epoch = epoch\n",
    "                print(f\"best_epoch: {best_epoch}\")\n",
    "                if _classify: \n",
    "                    if _attention:\n",
    "                        saved_model = {\n",
    "                            'VAE': VAE_network.state_dict(),\n",
    "                            'CLASS': CLASS_network.state_dict(),\n",
    "                            'ATT': ATT_Layer.state_dict(),\n",
    "                        }\n",
    "                    else:\n",
    "                        saved_model = {\n",
    "                            'VAE': VAE_network.state_dict(),\n",
    "                            'CLASS': CLASS_network.state_dict(),\n",
    "                        }\n",
    "                else:\n",
    "                    saved_model = {\n",
    "                            'VAE': VAE_network.state_dict(),\n",
    "                        }\n",
    "\n",
    "            # Early Stopping!\n",
    "            if curr_loss > prev_loss and epoch > 50:\n",
    "                print(f\"prev_loss: {prev_loss}, curr_loss: {curr_loss}\")\n",
    "                trigger_times += 1\n",
    "                print(f'Times without improved: {trigger_times}')\n",
    "\n",
    "                if trigger_times >= limit_stop:\n",
    "                    print(f'[*] Early stopping in Epoch: {epoch} !')\n",
    "                    print('Saving Model at epoch {}'.format(epoch+1))\n",
    "                    break\n",
    "            else:\n",
    "                trigger_times = 0\n",
    "                prev_loss = curr_loss\n",
    "\n",
    "        # Plot and save loss and accuracy data if required\n",
    "        if _plotLoss:\n",
    "            if _classify:\n",
    "                loss_acc_data = {\n",
    "                    \"train_loss\": train_loss_graph,\n",
    "                    \"valid_loss\": valid_loss_graph,\n",
    "                    \"train_acc\": train_acc_graph,\n",
    "                    \"valid_acc\": valid_acc_graph\n",
    "                }\n",
    "                with open(f'{_data}_step2_loss_acc_data_{model_name}_{_normalization}_Fold_{str(fold)}-A-{_att_method}-{_incerteza}-Loss-{_set_loss}.pkl', 'wb') as f:\n",
    "                    pickle.dump(loss_acc_data, f)\n",
    "            else:\n",
    "                loss_data = {\n",
    "                    \"train_loss\": train_loss_graph,\n",
    "                    \"valid_loss\": valid_loss_graph\n",
    "                }\n",
    "                with open(f'{_data}_step1_loss_data_{model_name}_{_normalization}_Fold_{str(fold)}-A-{_att_method}-{_incerteza}-Loss-{_set_loss}.pkl', 'wb') as f:\n",
    "                    pickle.dump(loss_data, f)\n",
    "\n",
    "        # Save models\n",
    "        if _classify:\n",
    "            print('Saving VAE and CLASS model!')\n",
    "            if _attention and _incerteza:\n",
    "                torch.save(saved_model['VAE'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "                torch.save(saved_model['CLASS'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "                torch.save(saved_model['ATT'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_ATT_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "            elif _incerteza == True and _attention == False:\n",
    "                torch.save(saved_model['VAE'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-I-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "                torch.save(saved_model['CLASS'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-I-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "            elif _attention == True and _incerteza == False:\n",
    "                torch.save(saved_model['VAE'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "                torch.save(saved_model['CLASS'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "                torch.save(saved_model['ATT'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_ATT_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "            else:\n",
    "                torch.save(saved_model['VAE'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-Loss-{_set_loss}-C.pth')\n",
    "                torch.save(saved_model['CLASS'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-Loss-{_set_loss}-C.pth')\n",
    "        else:\n",
    "            if _attention:\n",
    "                torch.save(saved_model['VAE'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_{model_name}_{_normalization}_{_augmentation}_A_{_att_method}_Fold_{str(fold)}.pth')\n",
    "            else:\n",
    "                torch.save(saved_model['VAE'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}.pth')\n",
    "\n",
    "    # Print fold results\n",
    "    print(f'K-FOLD CROSS VALIDATION RESULTS FOR {_k_folds} FOLDS')\n",
    "    print('--------------------------------')\n",
    "    sum = 0.0\n",
    "    for key, value in results.items():\n",
    "        print(f'Fold {key}: {value} %')\n",
    "        sum += value\n",
    "        print(f'Average: {sum/len(results.items())} %')\n",
    "\n",
    "\n",
    "print('Testing VAE+CLASS')\n",
    "\n",
    "with torch.no_grad():\n",
    "     # Initialize lists to store various evaluation metrics\n",
    "    accuracy = []\n",
    "    balancedAccuracyScore = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    f1 = []\n",
    "    auc = []\n",
    "    test_loss = []\n",
    "\n",
    "    try:\n",
    "        X_test_fold = X_test.drop(['Classe'], axis=1)\n",
    "    except:\n",
    "        X_test_fold = X_test.copy()\n",
    "\n",
    "    # Preprocess test data\n",
    "    X_test_fold, _ = preprocessing(X_test_fold, NORM, _normalization, _train=False)\n",
    "\n",
    "    if conv:\n",
    "        X_test_fold = np.expand_dims(X_test_fold, 1)\n",
    "\n",
    "    # Prepare test data loader\n",
    "    test_dataloader, test_size = prepare_data_loader(X_test_fold, y_test, batch_size=_batch_size, shuffle=False)\n",
    "    \n",
    "    for fold in range(_k_folds):\n",
    "        # Initialize VAE and ClassifyingNetwork models\n",
    "        VAE_network = VAE(latent_dims=_latent_dims).to(device) \n",
    "        num_ftrs = _latent_dims * 2\n",
    "        CLASS_network = ClassifyingNetwork(num_ftrs=num_ftrs).to(device)\n",
    "        \n",
    "        # Define model paths\n",
    "        model_path = './model/'\n",
    "        if _attention and _incerteza:\n",
    "            vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "            class_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "            att_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_ATT_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "        elif _incerteza == True and  _attention == False:\n",
    "            vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-I-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "            class_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-I-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "        elif _attention == True and _incerteza == False:\n",
    "            vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "            class_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "            att_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_ATT_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "        else:\n",
    "            vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-Loss-{_set_loss}-C.pth'\n",
    "            class_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-Loss-{_set_loss}-C.pth'\n",
    "\n",
    "        MSE_criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "        # Set the classification criterion\n",
    "        if _set_loss == \"focal_loss\":\n",
    "            print('Focal Loss Chosen!')\n",
    "            CLASS_criterion = FocalLoss(alpha=0.25, gamma=2.0, reduction='sum')\n",
    "        elif _set_loss == \"adaptative_focal_loss\":\n",
    "            print('Adaptative Focal Loss Chosen!')\n",
    "            CLASS_criterion = AdaptativeFocalLoss(alpha=0.25, gamma=2.0, reduction='sum')\n",
    "        elif _set_loss == \"cross_entropy_loss\":\n",
    "            print('Cross Entropy Loss Chosen!')\n",
    "            CLASS_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Invalid Loss: {_set_loss}\")\n",
    "\n",
    "        # Load the pre-trained models if they exist\n",
    "        if os.path.exists(vae_model_path) and os.path.exists(class_model_path):\n",
    "            print(f'VAE already exists. Loading model {vae_model_path}.')\n",
    "            VAE_network.load_state_dict(torch.load(vae_model_path, map_location = device))\n",
    "            VAE_network.eval()\n",
    "            print(f'CLASS already exists. Loading model {class_model_path}.')\n",
    "            CLASS_network.load_state_dict(torch.load(class_model_path, map_location = device))\n",
    "            CLASS_network.eval()\n",
    "            if _attention:\n",
    "                print(f'ATT Weights already exists. Loading model {att_model_path}.')\n",
    "                ATT_Layer.load_state_dict(torch.load(att_model_path, map_location = device))\n",
    "                ATT_Layer.eval()\n",
    "\n",
    "        # Initialize the joint model\n",
    "        JOINT_Model = JointModel(VAE_network, CLASS_network, ATT_Layer)\n",
    "\n",
    "        test_running_loss = 0.0\n",
    "        test_running_corrects = 0\n",
    "        test_acc = 0\n",
    "        ypredVector = []\n",
    "        labelsVector = []\n",
    "\n",
    "        # Test loop\n",
    "        for i, test_data in enumerate(test_dataloader):\n",
    "            inputs, labels = test_data\n",
    "            if conv:\n",
    "                inputs = inputs.to(device)\n",
    "            else:\n",
    "                inputs = inputs.to(device).squeeze(1)\n",
    "            labels = labels.type(dtype=torch.LongTensor)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            try:\n",
    "                labels = labels.squeeze(1)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Variational AutoEncoder NN\n",
    "            x_pred, mean, logvar = VAE_network.forward(inputs, encoder=False, decoder=True)\n",
    "            \n",
    "            TMSE_loss = MSE_criterion(x_pred, inputs)\n",
    "            \n",
    "            outputs = JOINT_Model(inputs, attention=_attention, incerteza=_incerteza, method=_att_method)\n",
    "\n",
    "            _, y_pred = torch.max(outputs, 1)\n",
    "\n",
    "            if _set_loss == \"adaptative_focal_loss\":\n",
    "                TCLASS_loss = CLASS_criterion(outputs, labels, torch.Tensor([1]))\n",
    "            else:\n",
    "                TCLASS_loss = CLASS_criterion(outputs, labels)\n",
    "                \n",
    "            valid_running_corrects += torch.sum(y_pred == labels.data)\n",
    "\n",
    "            ypredVector += y_pred.detach().cpu()\n",
    "            labelsVector += labels.data.detach().cpu()\n",
    "    \n",
    "            loss = TVAE_loss + (_gamma_3 * TCLASS_loss)\n",
    "\n",
    "            test_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # plot_gallery([inputs.detach().cpu(), x_pred.detach().cpu()], epoch, fold, 1, 2, all_plot=True)  \n",
    "\n",
    "        test_epoch_loss = test_running_loss / test_size['y_size'][0]\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        accuracy_fold = accuracy_score(labelsVector, ypredVector)\n",
    "        balancedAccuracyScore_fold = balanced_accuracy_score(labelsVector, ypredVector)\n",
    "        recall_fold = recall_score(labelsVector, ypredVector, average='weighted')\n",
    "        precision_fold = precision_score(labelsVector, ypredVector, average='weighted', zero_division=True)\n",
    "        f1_fold = f1_score(labelsVector, ypredVector, average='weighted')\n",
    "\n",
    "        test_loss.append(test_epoch_loss)\n",
    "        accuracy.append(accuracy_score(labelsVector, ypredVector)) \n",
    "        balancedAccuracyScore.append(balanced_accuracy_score(labelsVector, ypredVector)) \n",
    "        recall.append(recall_score(labelsVector, ypredVector, average='weighted'))\n",
    "        precision.append(precision_score(labelsVector, ypredVector, average='weighted', zero_division=True))\n",
    "        f1.append(f1_score(labelsVector, ypredVector, average='weighted'))\n",
    "\n",
    "\n",
    "        # Save results per fold\n",
    "        fold_csv_filename = f'./results_per_fold/{_data}_resultados_{model_name}_{_normalization}_{Sampling_mode}_per_fold.csv'\n",
    "        fold_results_data = {\n",
    "            'Fold': fold,\n",
    "            'Dataset': _dataset_name,\n",
    "            '_gamma_2': _gamma_2,\n",
    "            '_gamma_3': _gamma_3,\n",
    "            '_batch_size': _batch_size,\n",
    "            '_latent_dims': _latent_dims,\n",
    "            'Model': f'{model_name}_{_normalization}',\n",
    "            'Attention': _attention,\n",
    "            'Attention_Method': _att_method,\n",
    "            'Incerteza': _incerteza,\n",
    "            'Sampling Mode': Sampling_mode,\n",
    "            'Loss Function': _set_loss,\n",
    "            'Test - loss': test_epoch_loss,\n",
    "            'Test - Accuracy Score': accuracy_fold,\n",
    "            'Test - Balanced Accuracy Score': balancedAccuracyScore_fold,\n",
    "            'Test - Precision Score': recall_fold,\n",
    "            'Test - Recall Score': precision_fold,\n",
    "            'Test - F1 Score': f1_fold,\n",
    "        }\n",
    "        fold_header = fold_results_data.keys()\n",
    "\n",
    "        save_results_to_csv(fold_csv_filename, fold_results_data, fold_header)\n",
    "\n",
    "    # Save general results\n",
    "    general_csv_filename = f'./results/{_data}_resultados_{model_name}_{_normalization}_{Sampling_mode}.csv'\n",
    "    general_results_data = {\n",
    "        'Dataset': _dataset_name,\n",
    "        'Model': f'{model_name}_{_normalization}',\n",
    "        'Attention': _attention,\n",
    "        'Attention_Method': _att_method,\n",
    "        'Incerteza': _incerteza,\n",
    "        'Sampling Mode': Sampling_mode,\n",
    "        '_gamma_2': _gamma_2,\n",
    "        '_gamma_3': _gamma_3,\n",
    "        '_batch_size': _batch_size,\n",
    "        '_latent_dims': _latent_dims,\n",
    "        'Loss Function': _set_loss,\n",
    "        'loss - mean': np.mean(test_loss),\n",
    "        'loss - std': np.std(test_loss),\n",
    "        'Accuracy Score - mean': np.mean(accuracy),\n",
    "        'Accuracy Score - std': np.std(accuracy),\n",
    "        'Balanced Accuracy Score - mean': np.mean(balancedAccuracyScore),\n",
    "        'Balanced Accuracy Score - std': np.std(balancedAccuracyScore),\n",
    "        'Precision Score - mean': np.mean(precision),\n",
    "        'Precision Score - std': np.std(precision),\n",
    "        'Recall Score - mean': np.mean(recall),\n",
    "        'Recall Score - std': np.std(recall),\n",
    "        'F1 Score - mean': np.mean(f1),\n",
    "        'F1 Score - std': np.std(f1),\n",
    "    }\n",
    "    general_header = general_results_data.keys()\n",
    "\n",
    "    save_results_to_csv(general_csv_filename, general_results_data, general_header)\n",
    "\n",
    "    print(f\"Resultados salvos em ./results/{_data}_resultados_{model_name}_{_normalization}_{Sampling_mode}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script to run all models experiments\n",
    "\n",
    "This code executes multiple experiments, including:\n",
    "- Attention mechanism enabled/disabled\n",
    "- Gaussian neurons enabled/disabled\n",
    "- Loss function \n",
    "\n",
    "Some functions are repeated, allowing the experiments to be run after executing Optuna, without the need to execute the #Run section again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_file, step=1):\n",
    "    \"\"\"\n",
    "    Plots the loss function from a pickle file (.pkl).\n",
    "\n",
    "    Args:\n",
    "        loss_file (str): Path to the pickle file containing the loss function data.\n",
    "        step (int): Indicates the step of training. Default is 1.\n",
    "                    If step is 2, it also plots the training and validation accuracy.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Load the loss data from the pickle file\n",
    "    with open(loss_file, 'rb') as f:\n",
    "        loss_dict = pickle.load(f)\n",
    "    \n",
    "    # Extract accuracy values if step is 2\n",
    "    if step == 2:\n",
    "        train_acc_values = loss_dict['train_acc']\n",
    "        valid_acc_values = loss_dict['valid_acc']\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Extract loss values\n",
    "    train_loss_values = loss_dict['train_loss']\n",
    "    valid_loss_values = loss_dict['valid_loss']\n",
    "\n",
    "    # Extract the number of epochs\n",
    "    epochs = np.arange(1,len(loss_dict['train_loss'])+1)\n",
    "\n",
    "    # Plot the loss function\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.rcParams['legend.fontsize'] = 22\n",
    "    plt.rcParams.update({'font.size': 25})\n",
    "    plt.title(f\"Loss Function {model_name}_{_normalization}_Fold_{fold}\")\n",
    "    plt.plot(epochs, train_loss_values, label=\"Train_Loss\", linewidth=2, color='Blue')\n",
    "    plt.plot(epochs, valid_loss_values, label=\"Valid_Loss\", linewidth=2, color='Red')\n",
    "\n",
    "    # Plot accuracy if step is 2\n",
    "    if step == 2:\n",
    "        plt.plot(epochs, train_acc_values, label=\"Train_Accuracy\", linewidth=2, color='Green')\n",
    "        plt.plot(epochs, valid_acc_values, label=\"Valid_Accuracy\", linewidth=2, color='Orange')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(f\"./Loss_Plot/Loss_{model_name}_{_set_loss}_I-{_incerteza}_A-{_attention}-{_att_method}_{_normalization}_Fold_{fold}-Step{step}.pdf\")\n",
    "    # plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Example usage\n",
    "# loss_file = './Loss_Plot/pkls/loss_acc_data_22PC_BCA_in_GNA_VAE_cross_entropy_loss_I-False_A-False_None_Fold_0.pkl'\n",
    "# plot_loss(loss_file, step=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the normalization method\n",
    "_normalization = \"SNV\"\n",
    "\n",
    "# Define the path of general_csv_filename, to load the hyperparameters\n",
    "general_csv_filename = f'./results/{_data}_resultados_{model_name}_{_normalization}_{Sampling_mode}.csv'\n",
    "\n",
    "# Print the filename for verification\n",
    "general_csv_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Define hyperparameters\n",
    "_k_folds = 5  # Number of folds for K-fold cross-validation\n",
    "_lr = 1e-4  # Learning rate\n",
    "_epochs = 2000  # Number of training epochs\n",
    "_sched_factor = 0.1  # Factor by which the learning rate is reduced\n",
    "_sched_min_lr = 1e-6  # Minimum learning rate for the scheduler\n",
    "_sched_patience = 20  # Number of epochs with no improvement after which learning rate will be reduced\n",
    "_set_loss = \"cross_entropy_loss\"  # Loss function to be used (\"cross_entropy_loss\", \"adaptative_focal_loss\", \"focal_loss\")\n",
    "_gamma_1 = 1  # Weight for the reconstruction loss in the VAE\n",
    "_plotLoss = True  # Flag to indicate if the loss should be plotted\n",
    "\n",
    "\n",
    "class JointModel(nn.Module):\n",
    "    \"\"\"\n",
    "    A joint model combining a Variational Autoencoder (VAE), a Classifying Network, and an Attention Layer.\n",
    "\n",
    "    Parameters:\n",
    "    VAE (nn.Module): The Variational Autoencoder model.\n",
    "    ClassifyingNetwork (nn.Module): The Classifying Network model.\n",
    "    AttentionLayer (nn.Module): The Attention Layer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, VAE, ClassifyingNetwork, AttentionLayer):\n",
    "        super(JointModel, self).__init__()\n",
    "        self.vae = VAE\n",
    "        if ClassifyingNetwork is not None:\n",
    "            self.classifying_net = ClassifyingNetwork\n",
    "        if AttentionLayer is not None:\n",
    "            self.attention_layer = AttentionLayer\n",
    "        \n",
    "    def forward(self, x, attention=False, incerteza=False, method='Loung'):\n",
    "        \"\"\"\n",
    "        Forward pass for the JointModel.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): Input tensor.\n",
    "        attention (bool, optional): Flag to apply attention. Defaults to False.\n",
    "        incerteza (bool, optional): Flag to handle uncertainty. Defaults to False.\n",
    "        method (str, optional): Method for attention mechanism. Defaults to 'Loung'.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor after passing through the joint model.\n",
    "        \"\"\"\n",
    "        # Encode the input using the VAE to get mean and std deviation\n",
    "        mean_vae, std_dev = self.vae.forward(x)\n",
    "        ms_vector = torch.cat((mean_vae, std_dev), dim=1)\n",
    "\n",
    "        if incerteza:\n",
    "            # Handling uncertainty\n",
    "            weight_mean_nn = self.classifying_net.MLPclassify[1].weight\n",
    "            weight_std_nn = self.classifying_net.MLPclassify[1].weight\n",
    "            bias = self.classifying_net.MLPclassify[1].bias\n",
    "            \n",
    "            mean_vae_new = mean_vae.unsqueeze(2)\n",
    "            std_dev_new = std_dev.unsqueeze(2)\n",
    "\n",
    "            z_mean = torch.zeros_like(mean_vae_new)\n",
    "            z_std = torch.zeros_like(std_dev_new)\n",
    "\n",
    "            # Expand weights mean, std and biases using linear interpolation\n",
    "            weight_mean = torch.nn.functional.interpolate(\n",
    "                weight_mean_nn.unsqueeze(0).unsqueeze(0),\n",
    "                size=(mean_vae_new.size()[1], mean_vae_new.size()[1]),\n",
    "                mode='bilinear',\n",
    "                align_corners=False,\n",
    "            ).squeeze(0).squeeze(0)\n",
    "            \n",
    "            weight_std = torch.nn.functional.interpolate(\n",
    "                weight_std_nn.unsqueeze(0).unsqueeze(0),\n",
    "                size=(std_dev_new.size()[1], std_dev_new.size()[1]),\n",
    "                mode='bilinear',\n",
    "                align_corners=False,\n",
    "            ).squeeze(0).squeeze(0)\n",
    "            \n",
    "            bias = torch.linspace(bias.min().item(), bias.max().item(), steps=mean_vae_new.size()[1]).view(-1, 1).to(device)\n",
    "\n",
    "            # Increase the vector size to accommodate the repetition.\n",
    "            weight_mean = weight_mean.unsqueeze(0)\n",
    "            weight_std = weight_std.unsqueeze(0)\n",
    "            bias = bias.unsqueeze(0)\n",
    "\n",
    "            # Replicate the vector along axis 0.\n",
    "            repeated_vector_mean = weight_mean.repeat(mean_vae_new.size()[0], 1, 1)\n",
    "            repeated_vector_std = weight_std.repeat(std_dev_new.size()[0], 1, 1)\n",
    "            repeated_bias = bias.repeat(mean_vae_new.size()[0], 1, 1)\n",
    "\n",
    "            # Remove the additional dimension\n",
    "            weight_mean = repeated_vector_mean.squeeze(0)\n",
    "            weight_std = repeated_vector_std.squeeze(0)\n",
    "            bias = repeated_bias.squeeze(0)\n",
    "\n",
    "            for i in range(mean_vae_new.size()[0]): # Referente aos batchs\n",
    "                z_mean[i] = torch.matmul(weight_mean[i], mean_vae_new[i]) + bias[i]\n",
    "\n",
    "            for i in range(std_dev_new.size()[0]):\n",
    "                z_std[i] = torch.matmul(weight_std[i]**2, std_dev_new[i]**2)\n",
    "            \n",
    "        \n",
    "            z_mean = z_mean.squeeze(2)\n",
    "            z_std = z_std.squeeze(2)\n",
    "            \n",
    "            ms_vector = expected_sigm_of_norm(z_mean, z_std, method='probit')\n",
    "\n",
    "            if attention:\n",
    "                # Apply attention mechanism\n",
    "                attention_weights = self.attention_layer(ms_vector)\n",
    "                \n",
    "                ms_vector_new = ms_vector.clone()\n",
    "\n",
    "                if method == 'Loung':\n",
    "                    ms_vector_new *= attention_weights\n",
    "                elif method == 'Bahdanau':\n",
    "                    ms_vector_new += attention_weights\n",
    "\n",
    "                ms_vector = ms_vector_new\n",
    "        \n",
    "        # Pass the combined vector through the classifying network\n",
    "        output = self.classifying_net(ms_vector)\n",
    "        return output\n",
    "\n",
    "# Set classification and attention method flags\n",
    "_classify = True\n",
    "_att_method = 'Luong'\n",
    "\n",
    "# Loop through different loss functions\n",
    "for _set_loss in [\"cross_entropy_loss\", \"focal_loss\", \"adaptative_focal_loss\"]:\n",
    "    print(f\"Loss Running {_set_loss}!\")\n",
    "\n",
    "    # Loop through different uncertainty settings\n",
    "    for _incerteza in [False, True]:\n",
    "        print(f\"Running with Incerteza: {_incerteza}!\")\n",
    "\n",
    "        # Loop through different attention settings\n",
    "        for _attention in [False, True]:\n",
    "            print(f\"Running with Attention: {_attention}!\")\n",
    "\n",
    "            # Define the column names to look for in the CSV\n",
    "            column_names = ['_gamma_2', '_gamma_3', '_batch_size', '_latent_dims']\n",
    "\n",
    "            # Initialize the variables with default values\n",
    "            _gamma_2, _gamma_3, _batch_size, _latent_dims = None, None, None, None\n",
    "\n",
    "            # Open the CSV file for reading\n",
    "            with open(general_csv_filename, newline='') as csvfile:\n",
    "                reader = csv.DictReader(csvfile)\n",
    "\n",
    "                # Iterate through the rows of the CSV file\n",
    "                for row in reader:\n",
    "                    # Check if all the desired columns are present in the current row\n",
    "                    if all(col in row for col in column_names):\n",
    "                        # Convert the current row's values to the desired types\n",
    "                        if Sampling_mode == row['Sampling Mode']:\n",
    "                            _gamma_2 = float(row['_gamma_2'])\n",
    "                            _gamma_3 = float(row['_gamma_3'])\n",
    "                            _batch_size = int(row['_batch_size'])\n",
    "                            _latent_dims = int(row['_latent_dims'])\n",
    "                            break  # Stop searching after finding the first match\n",
    "            \n",
    "            if _gamma_2 is not None:\n",
    "                # Do something with the found values\n",
    "                print(f\"_gamma_2: {_gamma_2}, _gamma_3: {_gamma_3}, _batch_size: {_batch_size}, _latent_dims: {_latent_dims}\")\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Invalid, values not found in the CSV dataset.\")\n",
    "                \n",
    "            # For fold results\n",
    "            results = {}\n",
    "\n",
    "            # Define the K-fold Cross Validator\n",
    "            kfold = StratifiedKFold(n_splits=_k_folds, random_state=_seed, shuffle=True)\n",
    "\n",
    "            # Start print\n",
    "            print('--------------------------------')\n",
    "\n",
    "            print(\"Normal has been choice!\")\n",
    "            X = pd.concat([X_train, X_valid])\n",
    "            y = pd.concat([y_train, y_valid])\n",
    "\n",
    "            # Get subclass labels for training and validation data\n",
    "            try:\n",
    "                labels_subclasses_train_valid = pd.Categorical(X['Classe']).codes\n",
    "            except:\n",
    "                labels_subclasses_train_valid = y\n",
    "\n",
    "\n",
    "            torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "            # K-fold Cross Validation model evaluation\n",
    "            for fold, (train_index, valid_index) in enumerate(kfold.split(X, labels_subclasses_train_valid)):\n",
    "\n",
    "                # Print\n",
    "                print(f'FOLD {fold}')\n",
    "                print('--------------------------------')\n",
    "\n",
    "                # Obtém os dados de treinamento e validação para o fold atual\n",
    "                X_train_fold, X_valid_fold = X.iloc[train_index], X.iloc[valid_index]\n",
    "                y_train_fold, y_valid_fold = y.iloc[train_index], y.iloc[valid_index]\n",
    "\n",
    "                if Sampling_mode == 'None':\n",
    "                    print(\"None Oversampling mode has been chosen!\")\n",
    "                else:\n",
    "                    raise Exception(\"Sorry, not implemented yet!\")\n",
    "\n",
    "                try:\n",
    "                    X_train_fold = X_train_fold.drop(['Classe'], axis=1)\n",
    "                    X_valid_fold = X_valid_fold.drop(['Classe'], axis=1)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # Set normalization method\n",
    "                NORM = snv()  # Options: None, derivative(), sav_gol(), StandardScaler()\n",
    "        \n",
    "                # Preprocess the data\n",
    "                X_train_fold, NORM = preprocessing(X_train_fold, NORM, _normalization, _train=True)\n",
    "                X_valid_fold, _ = preprocessing(X_valid_fold, NORM, _normalization, _train=False)\n",
    "\n",
    "                if conv:\n",
    "                    X_train_fold = np.expand_dims(X_train_fold, 1)\n",
    "                    X_valid_fold = np.expand_dims(X_valid_fold, 1)\n",
    "\n",
    "                train_dataloader, train_size = prepare_data_loader(X_train_fold, y_train_fold, batch_size=_batch_size, shuffle=True)\n",
    "                valid_dataloader, valid_size = prepare_data_loader(X_valid_fold, y_valid_fold, batch_size=_batch_size, shuffle=False)\n",
    "                    \n",
    "                if _incerteza:\n",
    "                    input_attention_dims = _latent_dims\n",
    "                else:\n",
    "                    input_attention_dims = _latent_dims*2\n",
    "\n",
    "                if _attention:\n",
    "                    ATT_Layer = AttentionLayer(input_attention_dims, _latent_dims).to(device)\n",
    "                else:\n",
    "                    ATT_Layer = None\n",
    "\n",
    "                VAE_network = VAE(latent_dims=_latent_dims).to(device)\n",
    "\n",
    "                if _classify:\n",
    "                    CLASS_network = ClassifyingNetwork(num_ftrs=input_attention_dims).to(device)\n",
    "                else:\n",
    "                    CLASS_network = None\n",
    "\n",
    "                model_path = './model/'\n",
    "                vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}.pth'\n",
    "                \n",
    "                if os.path.exists(vae_model_path) and _classify:\n",
    "                    print(f'VAE already exists. Loading model {vae_model_path}.')\n",
    "                    VAE_network.load_state_dict(torch.load(vae_model_path, map_location=device))\n",
    "\n",
    "                JOINT_Model = JointModel(VAE_network, CLASS_network, ATT_Layer)\n",
    "\n",
    "                opt_parameters = list(VAE_network.parameters())\n",
    "\n",
    "                if _classify:\n",
    "                    opt_parameters += list(CLASS_network.parameters())\n",
    "                \n",
    "                if _attention:\n",
    "                    opt_parameters += list(ATT_Layer.parameters())\n",
    "\n",
    "                optimizer = torch.optim.Adam(opt_parameters, weight_decay=1e-3, lr=_lr)\n",
    "\n",
    "                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=_sched_factor, min_lr=_sched_min_lr, patience=_sched_patience)\n",
    "\n",
    "                MSE_criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "                if _classify:\n",
    "                    if _set_loss == \"focal_loss\":\n",
    "                        print('Focal Loss Chosen!')\n",
    "                        CLASS_criterion = FocalLoss(alpha=0.25, gamma=2.0, reduction='sum')\n",
    "                    elif _set_loss == \"adaptative_focal_loss\":\n",
    "                        print('Adaptative Focal Loss Chosen!')\n",
    "                        CLASS_criterion = AdaptativeFocalLoss(alpha=0.25, gamma=2.0, reduction='sum')\n",
    "                    elif _set_loss == \"cross_entropy_loss\":\n",
    "                        print('Cross Entropy Loss Chosen!')\n",
    "                        CLASS_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "                    else:\n",
    "                        raise NotImplementedError(f\"Invalid Loss: {_set_loss}\")\n",
    "                \n",
    "                print('Training VAE')\n",
    "                mean_train_loss = []\n",
    "                mean_valid_loss = []\n",
    "                curr_loss = 0\n",
    "                prev_loss = np.inf\n",
    "                train_acc = 0.0\n",
    "                valid_acc = 0.0\n",
    "                limit_stop = 20 #100\n",
    "                train_loss_graph = []\n",
    "                valid_loss_graph = []\n",
    "                train_acc_graph = []\n",
    "                valid_acc_graph = []\n",
    "\n",
    "                x_train_ftrs = torch.Tensor([]).to(device)\n",
    "                y_train_ftrs = torch.Tensor([]).to(device)\n",
    "                x_valid_ftrs = torch.Tensor([]).to(device)\n",
    "                y_valid_ftrs = torch.Tensor([]).to(device)\n",
    "\n",
    "                for epoch in range(_epochs):\n",
    "                    train_loss = []\n",
    "                    valid_loss = []\n",
    "                    running_loss = 0.0\n",
    "                    running_kld = 0.0\n",
    "                    running_recon = 0.0\n",
    "                    running_corrects = 0\n",
    "\n",
    "                    beta = torch.Tensor([epoch+1])\n",
    "                \n",
    "                    for i, train_data in enumerate(train_dataloader):\n",
    "\n",
    "                        inputs, labels = train_data\n",
    "                        \n",
    "                        if conv:\n",
    "                            inputs = inputs.to(device)\n",
    "                        else:\n",
    "                            inputs = inputs.to(device).squeeze(1)\n",
    "\n",
    "                        labels = labels.type(dtype=torch.LongTensor)\n",
    "                        labels = labels.to(device)\n",
    "\n",
    "                        try:\n",
    "                            labels = labels.squeeze(1)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Variational AutoEncoder NN\n",
    "                        inputs_pred, mean, logvar = VAE_network.forward(inputs, encoder=False, decoder=True)\n",
    "                        \n",
    "                        MSE_loss = MSE_criterion(inputs_pred, inputs) # Reconstruction Loss \n",
    "                        KLD_loss = - 0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "                        VAE_loss = (_gamma_1 * MSE_loss) + (_gamma_2 * KLD_loss)\n",
    "                        \n",
    "                        if _classify:\n",
    "                            outputs = JOINT_Model(inputs, attention=_attention, incerteza=_incerteza, method=_att_method)\n",
    "                            \n",
    "                            _, y_pred = torch.max(outputs, 1)\n",
    "\n",
    "                            if _set_loss == \"adaptative_focal_loss\":\n",
    "                                CLASS_loss = CLASS_criterion(outputs, labels, beta)\n",
    "                                beta += 1\n",
    "                            else:\n",
    "                                CLASS_loss = CLASS_criterion(outputs, labels)\n",
    "\n",
    "                            running_corrects += torch.sum(y_pred == labels.data)\n",
    "                            t_loss = VAE_loss + (_gamma_3 * CLASS_loss)\n",
    "                        else:\n",
    "                            t_loss =  VAE_loss\n",
    "\n",
    "                        running_recon += _gamma_1 * MSE_loss.item() * inputs.size(0)\n",
    "                        running_kld += _gamma_2 * KLD_loss.item() * inputs.size(0)\n",
    "                        running_loss += t_loss.item() * inputs.size(0)\n",
    "\n",
    "                        t_loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                    if _classify:\n",
    "                        train_acc = running_corrects.double() / train_size['y_size'][0]\n",
    "                    \n",
    "\n",
    "                    epoch_kld = running_kld / train_size['y_size'][0]\n",
    "                    epoch_recon = running_recon / train_size['y_size'][0]\n",
    "                    epoch_loss = running_loss / train_size['y_size'][0]\n",
    "                    train_loss_graph.append(epoch_loss)\n",
    "\n",
    "                    # print(\"Valid\")\n",
    "                    with torch.no_grad():\n",
    "                        valid_running_loss = 0.0\n",
    "                        valid_running_kld = 0.0\n",
    "                        valid_running_recon = 0.0\n",
    "                        valid_running_corrects = 0\n",
    "\n",
    "                        for i, valid_data in enumerate(valid_dataloader):\n",
    "\n",
    "                            inputs, labels = valid_data\n",
    "                            \n",
    "                            if conv:\n",
    "                                inputs = inputs.to(device)\n",
    "                            else:\n",
    "                                inputs = inputs.to(device).squeeze(1)\n",
    "                            \n",
    "                            labels = labels.type(dtype=torch.LongTensor)\n",
    "                            labels = labels.to(device)\n",
    "                            \n",
    "                            try:\n",
    "                                labels = labels.squeeze(1)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "\n",
    "                            # Variational AutoEncoder NN\n",
    "                            x_pred, mean, logvar = VAE_network.forward(inputs, encoder=False, decoder=True)\n",
    "                            MSE_loss = MSE_criterion(x_pred, inputs)\n",
    "                            KLD_loss = - 0.5 * torch.sum(1 + logvar - mean.pow(2) - logvar.exp())\n",
    "                            TVAE_loss = (_gamma_1 * MSE_loss) + (_gamma_2 * KLD_loss)\n",
    "\n",
    "                            if _classify:\n",
    "                                outputs = JOINT_Model(inputs, attention=_attention, incerteza=_incerteza, method=_att_method)\n",
    "                                    \n",
    "                                _, y_pred = torch.max(outputs, 1)\n",
    "                                \n",
    "                                if _set_loss == \"adaptative_focal_loss\":\n",
    "                                    CLASS_loss = CLASS_criterion(outputs, labels, beta)\n",
    "                                else:\n",
    "                                    CLASS_loss = CLASS_criterion(outputs, labels)\n",
    "\n",
    "                                valid_running_corrects += torch.sum(y_pred == labels.data)\n",
    "                                loss = TVAE_loss + (_gamma_3 * CLASS_loss)\n",
    "                            else:\n",
    "                                loss = TVAE_loss\n",
    "\n",
    "                            valid_running_loss += loss.item() * inputs.size(0)\n",
    "                            valid_running_recon += _gamma_1 * MSE_loss.item() * inputs.size(0)\n",
    "                            valid_running_kld += _gamma_2 * KLD_loss.item() * inputs.size(0)\n",
    "\n",
    "                        if epoch % 10 == 0 and not _classify:\n",
    "                            plot_gallery([inputs.detach().cpu(), x_pred.detach().cpu()], epoch, fold, model_name, 1, 2, all_plot=True)  \n",
    "\n",
    "                        valid_epoch_loss = valid_running_loss / valid_size['y_size'][0]\n",
    "                        valid_epoch_kld = valid_running_kld / valid_size['y_size'][0]\n",
    "                        valid_epoch_recon = valid_running_recon / valid_size['y_size'][0]\n",
    "                        valid_loss_graph.append(valid_epoch_loss)\n",
    "\n",
    "                        scheduler.step(valid_epoch_loss)\n",
    "                        scheduler.get_last_lr()\n",
    "                        curr_loss = valid_epoch_loss\n",
    "\n",
    "                    if _classify:\n",
    "                        valid_acc = valid_running_corrects.double() / valid_size['y_size'][0]\n",
    "                        results[fold] = 100.0 * valid_acc\n",
    "                        print(f'Epoch: {epoch}')\n",
    "                        print(f'Train loss: {epoch_loss}, Train Acc: {train_acc*100:.2f}')\n",
    "                        print(f'Valid loss: {valid_epoch_loss}, Valid  Acc: {valid_acc*100:.2f}')\n",
    "                    else:\n",
    "                        print(f'Epoch: {epoch} | Train loss: {epoch_loss} | Train KLD: {epoch_kld} | Train Recon Loss:{epoch_recon}')\n",
    "                        print(f'Epoch: {epoch} | Valid loss: {valid_epoch_loss} | Valid KLD: {valid_epoch_kld} | Valid Recon Loss:{valid_epoch_recon}')\n",
    "                        \n",
    "                    # Verifique se a loss atual é menor que a melhor até agora\n",
    "                    if curr_loss < prev_loss:\n",
    "                        best_epoch = epoch\n",
    "                        print(f\"best_epoch: {best_epoch}\")\n",
    "                        if _classify: \n",
    "                            if _attention:\n",
    "                                saved_model = {\n",
    "                                    'VAE': VAE_network.state_dict(),\n",
    "                                    'CLASS': CLASS_network.state_dict(),\n",
    "                                    'ATT': ATT_Layer.state_dict(),\n",
    "                                }\n",
    "                            else:\n",
    "                                saved_model = {\n",
    "                                    'VAE': VAE_network.state_dict(),\n",
    "                                    'CLASS': CLASS_network.state_dict(),\n",
    "                                }\n",
    "                        else:\n",
    "                            saved_model = {\n",
    "                                    'VAE': VAE_network.state_dict(),\n",
    "                                }\n",
    "\n",
    "\n",
    "                    # Early Stopping!\n",
    "                    if curr_loss > prev_loss and epoch > 50:\n",
    "                        print(f\"prev_loss: {prev_loss}, curr_loss: {curr_loss}\")\n",
    "                        trigger_times += 1\n",
    "                        print(f'Times without improved: {trigger_times}')\n",
    "\n",
    "                        if trigger_times >= limit_stop:\n",
    "                            print(f'[*] Early stopping in Epoch: {epoch} !')\n",
    "                            print('Saving Model at epoch {}'.format(epoch+1))\n",
    "                            break\n",
    "                    else:\n",
    "                        trigger_times = 0\n",
    "                        prev_loss = curr_loss\n",
    "\n",
    "                if _plotLoss:\n",
    "                    # Create a dictionary to store loss and accuracy data for training and validation\n",
    "                    loss_acc_data = {\n",
    "                        \"train_loss\": train_loss_graph,  # List of training loss values for each epoch\n",
    "                        \"valid_loss\": valid_loss_graph,  # List of validation loss values for each epoch\n",
    "                        \"train_acc\": train_acc_graph,    # List of training accuracy values for each epoch\n",
    "                        \"valid_acc\": valid_acc_graph     # List of validation accuracy values for each epoch\n",
    "                    }\n",
    "\n",
    "                    # Save the loss and accuracy data to a pickle file\n",
    "                    with open(f'./Loss_Plot/pkls/loss_acc_data_{model_name}_{_set_loss}_I-{_incerteza}_A-{_attention}-{_att_method}_{_normalization}_Fold_{fold}.pkl', 'wb') as f:\n",
    "                        pickle.dump(loss_acc_data, f)\n",
    "\n",
    "                    # Extract the loss values for plotting\n",
    "                    train_loss_values = loss_acc_data['train_loss']\n",
    "                    valid_loss_values = loss_acc_data['valid_loss']\n",
    "\n",
    "                    # Create an array of epoch numbers for the x-axis\n",
    "                    epochs = np.arange(1, len(loss_acc_data['train_loss']) + 1)\n",
    "\n",
    "                    # Plot the loss function\n",
    "                    plt.figure(figsize=(16, 10))  # Set the figure size\n",
    "                    plt.rcParams['legend.fontsize'] = 22  # Set the legend font size\n",
    "                    plt.rcParams.update({'font.size': 25})  # Update the font size\n",
    "                    plt.title(f\"Loss Function {model_name}_{_normalization}_Fold_{fold}\")  # Set the plot title\n",
    "                    plt.plot(epochs, train_loss_values, label=\"Train_Loss\", linewidth=2, color='Blue')  # Plot training loss\n",
    "                    plt.plot(epochs, valid_loss_values, label=\"Valid_Loss\", linewidth=2, color='Red')  # Plot validation loss\n",
    "                    plt.xlabel('Epoch')  # Set the x-axis label\n",
    "                    plt.ylabel('Loss')  # Set the y-axis label\n",
    "                    plt.legend()  # Display the legend\n",
    "                    plt.savefig(f\"./Loss_Plot/Loss_{model_name}_{_set_loss}_I-{_incerteza}_A-{_attention}-{_att_method}_{_normalization}_Fold_{fold}.pdf\")  # Save the plot to a PDF file\n",
    "                    plt.close()  # Close the plot\n",
    "\n",
    "\n",
    "                if _classify:\n",
    "                    print('Saving VAE and CLASS model!')\n",
    "                    if _attention == True and _incerteza == True:\n",
    "                        torch.save(saved_model['VAE'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "                        torch.save(saved_model['CLASS'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "                        torch.save(saved_model['ATT'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_ATT_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "                    elif _incerteza == True and _attention == False:\n",
    "                        torch.save(saved_model['VAE'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-I-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "                        torch.save(saved_model['CLASS'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-I-{_incerteza}-Loss-{_set_loss}-C.pth')\n",
    "                    elif _attention == True and _incerteza == False:\n",
    "                        torch.save(saved_model['VAE'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-Loss-{_set_loss}-C.pth')\n",
    "                        torch.save(saved_model['CLASS'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-Loss-{_set_loss}-C.pth')\n",
    "                        torch.save(saved_model['ATT'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_ATT_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-Loss-{_set_loss}-C.pth')\n",
    "                    else:\n",
    "                        torch.save(saved_model['VAE'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-Loss-{_set_loss}-C.pth')\n",
    "                        torch.save(saved_model['CLASS'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-Loss-{_set_loss}-C.pth')\n",
    "                else:\n",
    "                    if _attention:\n",
    "                        torch.save(saved_model['VAE'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_{model_name}_{_normalization}_{_augmentation}_A-{_att_method}_Fold_{str(fold)}.pth')\n",
    "                    else:\n",
    "                        torch.save(saved_model['VAE'], model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}.pth')\n",
    "\n",
    "            # Print fold results\n",
    "            print(f'K-FOLD CROSS VALIDATION RESULTS FOR {_k_folds} FOLDS')\n",
    "            print('--------------------------------')\n",
    "            sum = 0.0\n",
    "            for key, value in results.items():\n",
    "                print(f'Fold {key}: {value} %')\n",
    "                sum += value\n",
    "                print(f'Average: {sum/len(results.items())} %')\n",
    "\n",
    "            print('Testing VAE+CLASS')\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                accuracy = []\n",
    "                balancedAccuracyScore = []\n",
    "                recall = []\n",
    "                precision = []\n",
    "                f1 = []\n",
    "                auc = []\n",
    "                test_loss = []\n",
    "\n",
    "                try:\n",
    "                    X_test_fold = X_test.drop(['Classe'], axis=1)\n",
    "                except:\n",
    "                    X_test_fold = X_test.copy()\n",
    "\n",
    "                X_test_fold, _ = preprocessing(X_test_fold, NORM, _normalization, _train=False)\n",
    "\n",
    "                if conv:\n",
    "                    X_test_fold = np.expand_dims(X_test_fold, 1)\n",
    "\n",
    "                test_dataloader, test_size = prepare_data_loader(X_test_fold, y_test, batch_size=_batch_size, shuffle=False)\n",
    "                \n",
    "                for fold in range(_k_folds):\n",
    "                    \n",
    "                    VAE_network = VAE(latent_dims=_latent_dims).to(device)\n",
    "                        \n",
    "                                        \n",
    "                    if _incerteza:\n",
    "                        input_attention_dims = _latent_dims\n",
    "                    else:\n",
    "                        input_attention_dims = _latent_dims*2\n",
    "                    \n",
    "                    CLASS_network = ClassifyingNetwork(num_ftrs=input_attention_dims).to(device)\n",
    "                    \n",
    "                    model_path = './model/'\n",
    "                    if _attention == True and _incerteza == True:\n",
    "                        vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "                        class_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "                        att_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_ATT_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "                    elif _incerteza == True and  _attention == False:\n",
    "                        vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-I-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "                        class_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-I-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "                    elif _attention == True and _incerteza == False:\n",
    "                        vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-Loss-{_set_loss}-C.pth'\n",
    "                        class_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-Loss-{_set_loss}-C.pth'\n",
    "                        att_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_ATT_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-A-{_att_method}-Loss-{_set_loss}-C.pth'\n",
    "                    else:\n",
    "                        vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-Loss-{_set_loss}-C.pth'\n",
    "                        class_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_CLASS_{model_name}_{_normalization}_{_augmentation}_Fold_{str(fold)}-Loss-{_set_loss}-C.pth'\n",
    "\n",
    "                    MSE_criterion = nn.MSELoss(reduction='sum')\n",
    "\n",
    "                    if _set_loss == \"focal_loss\":\n",
    "                        print('Focal Loss Chosen!')\n",
    "                        CLASS_criterion = FocalLoss(alpha=0.25, gamma=2.0, reduction='sum')\n",
    "                    elif _set_loss == \"adaptative_focal_loss\":\n",
    "                        print('Adaptative Focal Loss Chosen!')\n",
    "                        CLASS_criterion = AdaptativeFocalLoss(alpha=0.25, gamma=2.0, reduction='sum')\n",
    "                    elif _set_loss == \"cross_entropy_loss\":\n",
    "                        print('Cross Entropy Loss Chosen!')\n",
    "                        CLASS_criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "                    else:\n",
    "                        raise NotImplementedError(f\"Invalid Loss: {_set_loss}\")\n",
    "\n",
    "                    if os.path.exists(vae_model_path) and os.path.exists(class_model_path):\n",
    "                        print(f'VAE already exists. Loading model {vae_model_path}.')\n",
    "                        VAE_network.load_state_dict(torch.load(vae_model_path, map_location = device))\n",
    "                        VAE_network.eval()\n",
    "                        print(f'CLASS already exists. Loading model {class_model_path}.')\n",
    "                        CLASS_network.load_state_dict(torch.load(class_model_path, map_location = device))\n",
    "                        CLASS_network.eval()\n",
    "                        if _attention:\n",
    "                            print(f'ATT Weights already exists. Loading model {att_model_path}.')\n",
    "                            ATT_Layer.load_state_dict(torch.load(att_model_path, map_location = device))\n",
    "                            ATT_Layer.eval()\n",
    "\n",
    "                    JOINT_Model = JointModel(VAE_network, CLASS_network, ATT_Layer)\n",
    "\n",
    "                    test_running_loss = 0.0\n",
    "                    test_running_corrects = 0\n",
    "                    test_acc = 0\n",
    "                    ypredVector = []\n",
    "                    labelsVector = []\n",
    "\n",
    "                    for i, test_data in enumerate(test_dataloader):\n",
    "\n",
    "                        inputs, labels = test_data\n",
    "                        if conv:\n",
    "                            inputs = inputs.to(device)\n",
    "                        else:\n",
    "                            inputs = inputs.to(device).squeeze(1)\n",
    "                        labels = labels.type(dtype=torch.LongTensor)\n",
    "                        labels = labels.to(device)\n",
    "                        \n",
    "                        try:\n",
    "                            labels = labels.squeeze(1)\n",
    "                        except:\n",
    "                            pass\n",
    "                        \n",
    "                        # Variational AutoEncoder NN\n",
    "                        x_pred, mean, logvar = VAE_network.forward(inputs, encoder=False, decoder=True)\n",
    "                        \n",
    "                        TMSE_loss = MSE_criterion(x_pred, inputs)\n",
    "                        \n",
    "                        outputs = JOINT_Model(inputs, attention=_attention, incerteza=_incerteza, method=_att_method)\n",
    "\n",
    "                        _, y_pred = torch.max(outputs, 1)\n",
    "\n",
    "                        if _set_loss == \"adaptative_focal_loss\":\n",
    "                            TCLASS_loss = CLASS_criterion(outputs, labels, torch.Tensor([1]))\n",
    "                        else:\n",
    "                            TCLASS_loss = CLASS_criterion(outputs, labels)\n",
    "                            \n",
    "                        valid_running_corrects += torch.sum(y_pred == labels.data)\n",
    "\n",
    "                        ypredVector += y_pred.detach().cpu()\n",
    "                        labelsVector += labels.data.detach().cpu()\n",
    "\n",
    "                        loss = TVAE_loss + (_gamma_3 * TCLASS_loss)\n",
    "\n",
    "                        test_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                        # plot_gallery([inputs.detach().cpu(), x_pred.detach().cpu()], epoch, fold, 1, 2, all_plot=True)  \n",
    "\n",
    "                    test_epoch_loss = test_running_loss / test_size['y_size'][0]\n",
    "\n",
    "                    # Calculate performance metrics\n",
    "                    accuracy_fold = accuracy_score(labelsVector, ypredVector)\n",
    "                    balancedAccuracyScore_fold = balanced_accuracy_score(labelsVector, ypredVector)\n",
    "                    recall_fold = recall_score(labelsVector, ypredVector, average='weighted')\n",
    "                    precision_fold = precision_score(labelsVector, ypredVector, average='weighted', zero_division=True)\n",
    "                    f1_fold = f1_score(labelsVector, ypredVector, average='weighted')\n",
    "\n",
    "                    # Append performance metrics to their respective lists\n",
    "                    test_loss.append(test_epoch_loss)\n",
    "                    accuracy.append(accuracy_score(labelsVector, ypredVector))\n",
    "                    balancedAccuracyScore.append(balanced_accuracy_score(labelsVector, ypredVector))\n",
    "                    recall.append(recall_score(labelsVector, ypredVector, average='weighted'))\n",
    "                    precision.append(precision_score(labelsVector, ypredVector, average='weighted', zero_division=True))\n",
    "                    f1.append(f1_score(labelsVector, ypredVector, average='weighted'))\n",
    "\n",
    "                    # Plot the confusion matrix\n",
    "                    plt.figure(figsize=(16, 10))  # Set the figure size\n",
    "                    plt.rcParams['legend.fontsize'] = 22  # Set the legend font size\n",
    "                    plt.rcParams.update({'font.size': 25})  # Update the font size\n",
    "\n",
    "                    # Calculate the confusion matrix\n",
    "                    conf_matrix = confusion_matrix(labelsVector, ypredVector)\n",
    "\n",
    "                    # Convert the confusion matrix to a DataFrame for better readability with seaborn\n",
    "                    DetaFrame_cm = pd.DataFrame(conf_matrix)\n",
    "\n",
    "                    # Plot the confusion matrix using seaborn\n",
    "                    sns.heatmap(DetaFrame_cm, annot=True, xticklabels=['BCA True', 'BCA False'], yticklabels=['BCA True', 'BCA False'], fmt='d')\n",
    "                    plt.title(f\"Confusion Matrix {model_name}_{_normalization}_Fold_{fold}\")  # Set the plot title\n",
    "\n",
    "                    # Save the confusion matrix plot to a PDF file\n",
    "                    plt.savefig(f\"./Confusion_Matrix/Folds/Confusion_Matrix_{model_name}_{_set_loss}_I-{_incerteza}_A-{_attention}-{_att_method}_{_normalization}_Fold_{fold}.pdf\")\n",
    "\n",
    "                    # plt.show()  # Uncomment this line to display the plot if running interactively\n",
    "                    plt.close()  # Close the plot\n",
    "\n",
    "                    # Save results per fold\n",
    "                    fold_csv_filename = f'./results_per_fold/{_data}_resultados_{model_name}_{_normalization}_{Sampling_mode}_per_fold.csv'\n",
    "                    fold_results_data = {\n",
    "                        'Fold': fold,\n",
    "                        'Dataset': _dataset_name,\n",
    "                        '_gamma_2': _gamma_2,\n",
    "                        '_gamma_3': _gamma_3,\n",
    "                        '_batch_size': _batch_size,\n",
    "                        '_latent_dims': _latent_dims,\n",
    "                        'Model': f'{model_name}_{_normalization}',\n",
    "                        'Attention': _attention,\n",
    "                        'Attention_Method': _att_method,\n",
    "                        'Incerteza': _incerteza,\n",
    "                        'Sampling Mode': Sampling_mode,\n",
    "                        'Loss Function': _set_loss,\n",
    "                        'Test - loss': test_epoch_loss,\n",
    "                        'Test - Accuracy Score': accuracy_fold,\n",
    "                        'Test - Balanced Accuracy Score': balancedAccuracyScore_fold,\n",
    "                        'Test - Precision Score': recall_fold,\n",
    "                        'Test - Recall Score': precision_fold,\n",
    "                        'Test - F1 Score': f1_fold,\n",
    "                    }\n",
    "                    fold_header = fold_results_data.keys()\n",
    "\n",
    "                    save_results_to_csv(fold_csv_filename, fold_results_data, fold_header)\n",
    "\n",
    "                # Save general results\n",
    "                general_csv_filename = f'./results/{_data}_resultados_{model_name}_{_normalization}_{Sampling_mode}.csv'\n",
    "                general_results_data = {\n",
    "                    'Dataset': _dataset_name,\n",
    "                    'Model': f'{model_name}_{_normalization}',\n",
    "                    'Attention': _attention,\n",
    "                    'Attention_Method': _att_method,\n",
    "                    'Incerteza': _incerteza,\n",
    "                    'Sampling Mode': Sampling_mode,\n",
    "                    '_gamma_2': _gamma_2,\n",
    "                    '_gamma_3': _gamma_3,\n",
    "                    '_batch_size': _batch_size,\n",
    "                    '_latent_dims': _latent_dims,\n",
    "                    'Loss Function': _set_loss,\n",
    "                    'loss - mean': np.mean(test_loss),\n",
    "                    'loss - std': np.std(test_loss),\n",
    "                    'Accuracy Score - mean': np.mean(accuracy),\n",
    "                    'Accuracy Score - std': np.std(accuracy),\n",
    "                    'Balanced Accuracy Score - mean': np.mean(balancedAccuracyScore),\n",
    "                    'Balanced Accuracy Score - std': np.std(balancedAccuracyScore),\n",
    "                    'Precision Score - mean': np.mean(precision),\n",
    "                    'Precision Score - std': np.std(precision),\n",
    "                    'Recall Score - mean': np.mean(recall),\n",
    "                    'Recall Score - std': np.std(recall),\n",
    "                    'F1 Score - mean': np.mean(f1),\n",
    "                    'F1 Score - std': np.std(f1),\n",
    "                }\n",
    "                general_header = general_results_data.keys()\n",
    "\n",
    "                save_results_to_csv(general_csv_filename, general_results_data, general_header)\n",
    "\n",
    "                print(f\"Resultados salvos em ./results/{_data}_resultados_{model_name}_{_normalization}_{Sampling_mode}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ReconData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    accuracy = []\n",
    "    balancedAccuracyScore = []\n",
    "    recall = []\n",
    "    precision = []\n",
    "    f1 = []\n",
    "    auc = []\n",
    "    test_loss = []\n",
    "\n",
    "    try:\n",
    "        X_test_fold = X_test.drop(['Classe'], axis=1)\n",
    "    except KeyError:\n",
    "        X_test_fold = X_test.copy()\n",
    "    \n",
    "    # NORM = derivative()\n",
    "    # NORM = snv()\n",
    "    # NORM = SNVTransformer()\n",
    "    X_test_fold, _ = preprocessing(X_test_fold, NORM, _normalization, _train=False)\n",
    "\n",
    "    if conv:\n",
    "        X_test_fold = np.expand_dims(X_test_fold, 1)\n",
    "\n",
    "    test_dataloader, test_size = prepare_data_loader(X_test_fold, y_test, batch_size=_batch_size, shuffle=False)\n",
    "        \n",
    "    VAE_network = VAE(latent_dims=_latent_dims).to(device)\n",
    "    \n",
    "    model_path = './model/'\n",
    "    _set_loss = 'focal_loss'\n",
    "    # Only Vae\n",
    "    # vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_{model_name}_{_normalization}_{_augmentation}_Fold_4.pth'\n",
    "    vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/NIR-SC-UFES_MS_E_MLP-IR_CNN-1D_SNV_None_Fold_4.pth'\n",
    "    print(vae_model_path)\n",
    "    # Full model VAE\n",
    "    # vae_model_path = model_path + f'Kfold_results/{Sampling_mode}/{_data}_MS_VAE_{model_name}_{_normalization}_{_augmentation}_Fold_3-A-{_att_method}-I-{_incerteza}-Loss-{_set_loss}-C.pth'\n",
    "    MSE_criterion = nn.MSELoss(reduction='sum')\n",
    "    # print(vae_model_path)\n",
    "\n",
    "    if os.path.exists(vae_model_path):\n",
    "        print(f'VAE already exists. Loading model {vae_model_path}.')\n",
    "        VAE_network.load_state_dict(torch.load(vae_model_path, map_location = device))\n",
    "        VAE_network.eval()\n",
    "\n",
    "    test_running_loss = 0.0\n",
    "    test_running_corrects = 0\n",
    "    test_acc = 0\n",
    "    ypredVector = []\n",
    "    labelsVector = []\n",
    "\n",
    "    for i, test_data in enumerate(test_dataloader):\n",
    "\n",
    "        inputs, labels = test_data\n",
    "        if conv:\n",
    "            inputs = inputs.to(device)\n",
    "        else:\n",
    "            inputs = inputs.to(device).squeeze(1)\n",
    "            \n",
    "        labels = labels.type(dtype=torch.LongTensor)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        try:\n",
    "            labels = labels.squeeze(1)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Variational AutoEncoder NN\n",
    "        x_pred, mean, logvar = VAE_network.forward(inputs, encoder=False, decoder=True)\n",
    "\n",
    "        eixo_x = [908.1, 914.294, 920.489, 926.683, 932.877, 939.072, \n",
    "          945.266, 951.46, 957.655, 963.849, 970.044, 976.238, \n",
    "          982.432, 988.627, 994.821, 1001.015, 1007.21, 1013.404, \n",
    "          1019.598, 1025.793, 1031.987, 1038.181, 1044.376, 1050.57, \n",
    "          1056.764, 1062.959, 1069.153, 1075.348, 1081.542, 1087.736, \n",
    "          1093.931, 1100.125, 1106.319, 1112.514, 1118.708, 1124.902, \n",
    "          1131.097, 1137.291, 1143.485, 1149.68, 1155.874, 1162.069, \n",
    "          1168.263, 1174.457, 1180.652, 1186.846, 1193.04, 1199.235, \n",
    "          1205.429, 1211.623, 1217.818, 1224.012, 1230.206, 1236.401, \n",
    "          1242.595, 1248.789, 1254.984, 1261.178, 1267.373, 1273.567, \n",
    "          1279.761, 1285.956, 1292.15, 1298.344, 1304.539, 1310.733, \n",
    "          1316.927, 1323.122, 1329.316, 1335.51, 1341.705, 1347.899, \n",
    "          1354.094, 1360.288, 1366.482, 1372.677, 1378.871, 1385.065, \n",
    "          1391.26, 1397.454, 1403.648, 1409.843, 1416.037, 1422.231, \n",
    "          1428.426, 1434.62, 1440.814, 1447.009, 1453.203, 1459.398, \n",
    "          1465.592, 1471.786, 1477.981, 1484.175, 1490.369, 1496.564, \n",
    "          1502.758, 1508.952, 1515.147, 1521.341, 1527.535, 1533.73, \n",
    "          1539.924, 1546.119, 1552.313, 1558.507, 1564.702, 1570.896, \n",
    "          1577.09, 1583.285, 1589.479, 1595.673, 1601.868, 1608.062, \n",
    "          1614.256, 1620.451, 1626.645, 1632.839, 1639.034, 1645.228, \n",
    "          1651.423, 1657.617, 1663.811, 1670.006, 1676.2]\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(16,10))\n",
    "\n",
    "        plt.rcParams['legend.fontsize'] = 22\n",
    "        plt.rcParams.update({'font.size': 25})\n",
    "        # ax.axis(\"off\")\n",
    "        plt.plot(eixo_x, inputs[2][0].squeeze(0).cpu(), linewidth=3, c='g')\n",
    "        plt.plot(eixo_x, x_pred[2][0].squeeze(0).cpu(), linewidth=3, c='r')\n",
    "\n",
    "        plt.legend(['Original', 'Reconstruído'])    \n",
    "        # plt.xlabel(\"Wavenumber (nm)\")\n",
    "        # plt.xlabel(\"Principal Components\")\n",
    "        # plt.ylabel(\"Absorption Level\")\n",
    "        plt.xlabel(\"Comprimento de onda (nm)\")\n",
    "        plt.ylabel(\"Absorbância\")\n",
    "        # plt.savefig(f'./Orig_NIR-SC-UFES_in_GNA_VAE.pdf')\n",
    "        # dir_save = '/mnt/hdd/matheusbecali/E-MLP/t-SNE/Orig_Recon_NIR-SC-UFES_in_GNA_VAE.pdf'\n",
    "        dir_save = '/mnt/hdd/matheusbecali/E-MLP/plots_imgs/Dissertarion/Orig_Recon_NIR-SC-UFES_in_GNA_VAE.pdf'\n",
    "        plt.savefig(dir_save)\n",
    "        plt.show()\n",
    "        # plt.close()\n",
    "        # plot_gallery([inputs.detach().cpu(), x_pred.detach().cpu()], epoch, fold, 1, 2, all_plot=True)\n",
    "        break  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('mbrenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6b4903e38b268ba3ee8a38c331067ca5e58b01cfd52a3afd9d406012d18b34f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
